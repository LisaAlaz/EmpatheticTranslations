{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRUpretrained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3A7TtZcrvt1"
      },
      "source": [
        "#2. GRU encoder-decoder with attention (pre-trained embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paqiRlAx7SMU"
      },
      "source": [
        "The following architecture is composed of of a GRU encoder and a GRU decoder, with an attention layer. Again we use MLP attention and the energy formula $s_i = a^T tanh (W_d d_t + W_s h_i)$. We use pretrained embeddings from BERT base uncased. The dimension of the embedding layer is 512 for both the encoder and the decoder. Again we use Adam optimiser with a learning rate of 0.001,cross-entropy loss and 0.5 dropout. Both models regardless of their input use the same vocabulary and have 72,848,954 tunable parameters. Both models are trained for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZLXFKnwu-fM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b966db4-058b-49bf-a2bb-bd50ece4c8e5"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\r\u001b[K     |▏                               | 10kB 22.6MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 30.6MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 22.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 17.9MB/s eta 0:00:01\r\u001b[K     |▊                               | 51kB 15.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 12.1MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 13.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 81kB 14.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 92kB 13.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 102kB 14.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 112kB 14.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 122kB 14.3MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 14.3MB/s eta 0:00:01\r\u001b[K     |██                              | 143kB 14.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 153kB 14.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 163kB 14.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 174kB 14.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 184kB 14.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 194kB 14.3MB/s eta 0:00:01\r\u001b[K     |███                             | 204kB 14.3MB/s eta 0:00:01\r\u001b[K     |███                             | 215kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 225kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 235kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 245kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 256kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 266kB 14.3MB/s eta 0:00:01\r\u001b[K     |████                            | 276kB 14.3MB/s eta 0:00:01\r\u001b[K     |████                            | 286kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 296kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 307kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 317kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 327kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 337kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 348kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 358kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 368kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 378kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 389kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 399kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 409kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 419kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 430kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 440kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 450kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 460kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 471kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 481kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 491kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 501kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 512kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 522kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 532kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 542kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 552kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 563kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 573kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 583kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 593kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 604kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 614kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 624kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 634kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 645kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 655kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 665kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 675kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 686kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 696kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 706kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 716kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 727kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 737kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 747kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 757kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 768kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 778kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 788kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 798kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 808kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 819kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 829kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 839kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 849kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 860kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 870kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 880kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 890kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 901kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 911kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 921kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 931kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 942kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 952kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 962kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 972kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 983kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 993kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.4MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.5MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.6MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.7MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.8MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.9MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.0MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.1MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.2MB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.3MB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 13.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Yw6Wk1u_c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7da0b35-7656-49f9-9208-72992e022ea9"
      },
      "source": [
        "#import statements\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator, Dataset, Example, LabelField\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel , Adafactor\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayPAcm0qvF86"
      },
      "source": [
        "#set seed\n",
        "SEED = 42\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPikOKcIxAEX"
      },
      "source": [
        "##2a) Non-empathetic response as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiQlbG20vJx_"
      },
      "source": [
        "#import csv file into pandas dataframe\n",
        "data_df = pd.read_csv('drive/MyDrive/data.csv', encoding=\"latin1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96Hey1xvMjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5ee553-4bfc-419a-d0d2-09559ff88239"
      },
      "source": [
        "#define device\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print(f'Found GPU at: {device_name}')\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU in use:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('using the CPU')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "GPU in use: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7bKNAyHvVCm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181,
          "referenced_widgets": [
            "8340e19291b4444ebfa7a9eed8023391",
            "1f070613710f4768b08d25acff04a29e",
            "34918a93bfac40b3aca191ee31d72ecf",
            "ca1d9b8aef3e4daea30b53177d9ed84c",
            "cfd3cf2934484d2d9c4fca07bf63007e",
            "9ef1efe53c15405eb6ff10adb806b937",
            "c830244cdc234ae8b09548acb7f0f08c",
            "25fa618e225c49febcc916b79694e7f6",
            "cecf9294509f4df48cb568ac6c68e104",
            "7ca6a76bcfea4724a3ab28d04c32850c",
            "d9917d87d6b24895aba88552ecb87abf",
            "8e176ed2e7724459b2df5cd1a1f423e9",
            "90504bceab6044f4b6b27fb591e16bb2",
            "00aa5fc5ac3445d19e90b142bc2fe9bd",
            "28ef7c456d0249bca7ee62f85f29f8dc",
            "b9771021d18440c2a9e589df6a5b03c4",
            "552196318db04a7c962a0d55bcd6152e",
            "8e704ad44a6248cf87407d5a6e3103dd",
            "be2499186ecc46afb77f4b918205e798",
            "f1f1949831e7465d8589c9bc6aace626",
            "9dc472c2e7004f8ba9b89824085b1e90",
            "6237dcc2912f4ec992a854234fae7df4",
            "11c61ddb06344249869945e8c0e1d751",
            "5b4e24b0f23046148f35bad1ad766eaa"
          ]
        },
        "outputId": "69111353-9949-462c-df64-6dfe7193b3dd"
      },
      "source": [
        "#embeddings\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "vocab_size = len( BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True).get_vocab() )\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8340e19291b4444ebfa7a9eed8023391",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cecf9294509f4df48cb568ac6c68e104",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "552196318db04a7c962a0d55bcd6152e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fANJT4OvY_V"
      },
      "source": [
        "# extra preprocessing steps\n",
        "# prepend CLS and append SEP, truncate, pad\n",
        "\n",
        "#parameters for the dataset and dataloader\n",
        "BATCH_SIZE = 32\n",
        "max_sent_length= 150\n",
        "\n",
        "#################################### Preprocessing function ####################################\n",
        "def preprocessing(df):\n",
        "\n",
        "    #get the source and target sentences\n",
        "    source_sentences = df.source.values\n",
        "    target_sentences = df.target.values\n",
        "\n",
        "    # initialize the tokenizer for tokenize the sentence and obtain the indexed list using bert vocabulary\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
        "    \n",
        "    encoded_source_sentences , encoded_target_sentences = [] , []\n",
        "    # iterate over every source , target sentence and tokenize them\n",
        "    for s_sent , t_sent in zip(source_sentences , target_sentences ):\n",
        "        #source sentence tokenization\n",
        "        s_encoded_sent = tokenizer.encode(\n",
        "                            s_sent,\n",
        "                            add_special_tokens = True,\n",
        "                            truncation=True,\n",
        "                            max_length = max_sent_length\n",
        "                    )\n",
        "        # target sentence tokenization\n",
        "        t_encoded_sent = tokenizer.encode(\n",
        "                            t_sent,\n",
        "                            add_special_tokens = True,\n",
        "                            truncation=True,\n",
        "                            max_length = max_sent_length\n",
        "                    )\n",
        "\n",
        "\n",
        "        encoded_source_sentences.append(s_encoded_sent)\n",
        "        encoded_target_sentences.append(t_encoded_sent)\n",
        "\n",
        "\n",
        "    \n",
        "    return encoded_source_sentences, encoded_target_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg6nztvpvaf-"
      },
      "source": [
        "#split into train, val and test sets (80-10-10)\n",
        "train_df, val_df, test_df = \\\n",
        "              np.split(data_df.sample(frac=1, random_state=42), \n",
        "                       [int(.8*len(data_df)), int(.9*len(data_df))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFn5E1Bfvdp3"
      },
      "source": [
        "# get the encoded source and target sentence\n",
        "train_source_sentences, train_target_sentences = preprocessing(train_df)\n",
        "val_source_sentences, val_target_sentences = preprocessing(val_df)\n",
        "test_source_sentences, test_target_sentences = preprocessing(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2fJndTovgKj"
      },
      "source": [
        "#define the torch dataset class to feed into the model\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    # class initialization with max sequence len , target and source sentence list\n",
        "    def __init__(self, data_list, target_list, max_sent_length=128):\n",
        "        \"\"\"\n",
        "        @param data_list: list of data tokens \n",
        "        @param target_list: list of data targets \n",
        "        \"\"\"\n",
        "        self.data_list = data_list   # source sentence list\n",
        "        self.target_list = target_list  # target sentence list\n",
        "        self.max_sent_length = max_sent_length   # maximum sentence lenght\n",
        "        assert (len(self.data_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "        \n",
        "    # return the source and target sentences\n",
        "    def __getitem__(self, key, max_sent_length=None):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        if max_sent_length is None:\n",
        "            max_sent_length = self.max_sent_length\n",
        "        token_idx = self.data_list[key][:max_sent_length]\n",
        "        label = self.target_list[key]\n",
        "        return [token_idx, label]\n",
        "\n",
        "    # we use this collate_fn to dynamically pad the batch according to the maximum len inside the sentences\n",
        "    def spam_collate_func(self,batch):\n",
        "        \"\"\"\n",
        "        Customized function for DataLoader that dynamically pads the batch so that all \n",
        "        data have the same length\n",
        "        \"\"\" \n",
        "        data_list = [] \n",
        "        target_list = []\n",
        "        mask_list = []\n",
        "\n",
        "        max_batch_seq_len = None # the length of longest sequence in batch\n",
        "                                 # if it is less than self.max_sent_length\n",
        "                                 # else max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        \"\"\"\n",
        "          # Pad the sequences in your data \n",
        "          # if their length is less than max_batch_seq_len\n",
        "          # or trim the sequences that are longer than self.max_sent_length\n",
        "          # return padded data_list and label_list\n",
        "        \"\"\"\n",
        "        \n",
        "        # find the max sequence length from the batch\n",
        "        max_batch_s_len = max(len(datum[0]) for datum in batch)\n",
        "        max_batch_t_len = max(len(datum[1]) for datum in batch)\n",
        "\n",
        "        #get the maximum from source and target lens\n",
        "        max_batch_seq_len = max(  [  max_batch_s_len , max_batch_t_len ] )\n",
        "\n",
        "        if max_batch_seq_len > self.max_sent_length:\n",
        "          max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        # pad each of the texts in batch wit constant 0\n",
        "        for datum in batch:\n",
        "          # padding the source vector\n",
        "          padded_s_vec = np.pad(np.array(datum[0]),\n",
        "                              pad_width = ((0, max_batch_seq_len - len(datum[0]))),\n",
        "                              mode = \"constant\", constant_values = 0)\n",
        "          # padding the target vector\n",
        "          padded_t_vec = np.pad(np.array(datum[1]),\n",
        "                              pad_width = ((0, max_batch_seq_len - len(datum[1]))),\n",
        "                              mode = \"constant\", constant_values = 0)\n",
        "\n",
        "          data_list.append(padded_s_vec)\n",
        "          target_list.append( padded_t_vec )\n",
        "\n",
        "\n",
        "        # convert to torch tensors \n",
        "        data_list = torch.from_numpy(np.array(data_list))\n",
        "        label_list = torch.from_numpy( np.array(target_list)  )\n",
        "\n",
        "        return [data_list, label_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7IjYB0hvmEr"
      },
      "source": [
        "#define the train dataset and the data loader \n",
        "train_dataset = Seq2SeqDataset( train_source_sentences , train_target_sentences , max_sent_length)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# define the validation dataset and the dataloader\n",
        "valid_dataset = Seq2SeqDataset( val_source_sentences , val_target_sentences , train_dataset.max_sent_length)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=False)\n",
        "\n",
        "# define the test dataset and the dataloader\n",
        "test_dataset = Seq2SeqDataset( test_source_sentences , test_target_sentences , train_dataset.max_sent_length)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyGELMU_vrSp"
      },
      "source": [
        "#define device cuda or cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWIcxmkQvsFq"
      },
      "source": [
        "#encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        # embedding layer \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5a-zjdrxLTx"
      },
      "source": [
        "#attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gahJtMtxxOXr"
      },
      "source": [
        "#decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVAVmIQ_xSHH"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pceqcXLxUMt"
      },
      "source": [
        "#hyperparams\n",
        "INPUT_DIM = vocab_size\n",
        "OUTPUT_DIM = vocab_size\n",
        "ENC_EMB_DIM = 512\n",
        "DEC_EMB_DIM = 512\n",
        "ENC_HID_DIM = 256\n",
        "DEC_HID_DIM = 256\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOsbqAWxZDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b15736f2-108e-466d-fbf1-f660e9148d1e"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(30522, 512)\n",
              "    (rnn): GRU(512, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
              "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(30522, 512)\n",
              "    (rnn): GRU(1024, 256)\n",
              "    (fc_out): Linear(in_features=1280, out_features=30522, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Vl9v8cxfrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f7ec28-22d2-4842-b9d4-68107f3c7884"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(30522, 512)\n",
              "    (rnn): GRU(512, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
              "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(30522, 512)\n",
              "    (rnn): GRU(1024, 256)\n",
              "    (fc_out): Linear(in_features=1280, out_features=30522, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQr6Bl-vxhEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3696248c-ff10-4bb2-fa72-bbc3bcf3fe51"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 72,848,954 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VQlsV_Gxj13"
      },
      "source": [
        "#define optimizer\n",
        "optimizer = optim.Adam(model.parameters() , lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YHzJFjpxlz-"
      },
      "source": [
        "#define loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2WOezZmxppf"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, ( src , trg ) in enumerate(iterator):\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        #move to device\n",
        "        src , trg = src.to(device) , trg.to(device)\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwMJD_lOxu_9"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src , trg) in enumerate(iterator):\n",
        "\n",
        "            #move to device\n",
        "            src , trg = src.to(device) , trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFlsxBhAx_aR"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCdyGs8UyCDe"
      },
      "source": [
        "#training loop\n",
        "\n",
        "N_EPOCHS = 100\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Training the model...\")\n",
        "tr = {'loss': [], 'PPL': []}\n",
        "val = {'loss': [], 'PPL': []}\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader , criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'drive/MyDrive/tut3-model-2a.pt')\n",
        "\n",
        "    # store logs\n",
        "    tr['loss'].append(train_loss)\n",
        "    tr['PPL'].append(math.exp(train_loss))\n",
        "    val['loss'].append(valid_loss)\n",
        "    val['PPL'].append(math.exp(valid_loss))\n",
        "    \n",
        "    if( (epoch+1)%10==0 ):\n",
        "      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnbTQQL8c4YO"
      },
      "source": [
        "#plot train and validation PPL logs\n",
        "\n",
        "fig = plt.figure(figsize=(10,7))\n",
        "\n",
        "y1 = tr['PPL']\n",
        "y2 = val['PPL']\n",
        "\n",
        "plt.plot(y1, \"-b\", label=\"Train perplexity\")\n",
        "plt.plot(y2, \"-r\", label=\"Validation perplexity\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.ylim(0, 550)\n",
        "plt.ylabel('Perplexity', fontsize=12)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.title(\"Train and validation perplexity \\n GRU with attention (BERT base embeddings) - Non-empathetic response input\", fontsize = 14)\n",
        "\n",
        "fig.savefig(\"2a-ppl.pdf\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PQsJoC-zMXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ed9232-1765-408e-ddce-e510ddfc5497"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/tut3-model-2a.pt'))\n",
        "\n",
        "test_loss_2a = evaluate(model, test_loader, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss_2a:.3f} | Test PPL: {math.exp(test_loss_2a):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 5.201 | Test PPL: 181.397 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkevobN1zSz6"
      },
      "source": [
        "def generate_sentence(model, iterator):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    pred_trg_pairs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src , trg) in enumerate(iterator):\n",
        "\n",
        "            #move to device\n",
        "            src , trg = src.to(device) , trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[:,1:]\n",
        "            trg = trg[:,1:]\n",
        "\n",
        "            output = output.detach().cpu().numpy()\n",
        "            trg = trg.detach().cpu().numpy()\n",
        "            pred= np.argmax( output , axis = -1 )\n",
        "\n",
        "            pred = tokenizer.batch_decode( pred , skip_special_tokens=True)\n",
        "            trg = tokenizer.batch_decode( trg , skip_special_tokens = True )\n",
        "\n",
        "            for i_pred , i_trg in zip( pred , trg  ):\n",
        "\n",
        "              print( \"Target Sentence : {} \\n\".format( i_trg )  )\n",
        "              print( \"Predicted Sentence : {} \\n\".format( i_pred )  )\n",
        "\n",
        "            break      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePXk0HslzWkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c103ac-1595-4ed9-e509-de45ad45488a"
      },
      "source": [
        "generate_sentence(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target Sentence : . must have meant so much to give yourself to a relationship where that loss hurts this bad. crap, that's quite the level of commitment!, that is something to hang your hat on. have that in you, right? did it once before. shit hurts right now but you have it in you!!. cant imagine how happy you will be when you find the next one... maybe one that fits the possibility of a promising relationship?, think you have a good base here. \n",
            "\n",
            "Predicted Sentence :  \n",
            "\n",
            "Target Sentence : feel only bad things, too. can laugh and joke with my buddies, but it just feels hollow and empty on the inside. even come to think, that often times smile and laugh, cause know a normal person would do this and did it once, so to don't look like a freak, do it at the right times.'m never excited these days. can't even remember the last time was. when decided to lose my virginity to a prostitute when was around 30, had some concerns but wasn't excited. and partys, knew couldn't wait for some when was in my early twenties., \n",
            "\n",
            "Predicted Sentence : . would do this and did it once, so to don't look like a, do it at the right times.'m never excited these days. can't even remember the last time was. when decided to lose my to a. when was around miles, had some is but wasn't excited. and partys, knew couldn't live for some when was in my., actually have to push myself to go out and of friends, even though age.ly it will be a nice evening.,, can't give you any advice. : \n",
            "\n",
            "Target Sentence : know. how feel. can barely survive now, never make it. \n",
            "\n",
            "Predicted Sentence : know................... excited.......... \n",
            "\n",
            "Target Sentence : ' ve come to help you. can't do much. - they're all lies. can understand how you feel because am exactly in your shoes. kill myself'm scared that'll go to hell.'m stuck. can assure you that you are not alone. if you want, you just got to hold on. \n",
            "\n",
            "Predicted Sentence : ' ve come to help you. can you that you are not alone. if you want, you just got to hold on........................................................................ \n",
            "\n",
            "Target Sentence : man, you're saying is actually quite common of someone who is good at art. people with a high degree or artistic talent are really good at recognizing good art, but get frustrated because their own art doesn't live up to the crazy high standard they've set for themselves. suggests that it takes 10, 000 hours to gain mastery in something. is a skill just like anything else. think a lot of people get fooled into this either have it or you don't mentality when it comes to creative arts.'s just not the case. think it's more important that you actually have an obsession and something that you want to be good at! people don \n",
            "\n",
            "Predicted Sentence : man, you're saying is actually quite common of someone who is good at. either have it or you don't mental don when it comes to creative a.'s just not the case. think it's more important that you actually have an and something that you want to be good at! people don't have that. keep practicing and never give up. you have the used you will succeed eventually.'ll get better and better every day........................ \n",
            "\n",
            "Target Sentence : swear, if people didn't have families, there would be a lot less depression in this world. guess it really depends on the type of family stress. you realistically put some distance between yourself and the stress? is it something that you have to resolve? \n",
            "\n",
            "Predicted Sentence : , if people didn't have for, there would be a lot less depression in this world. guess it really depends on the, of family stress. you notally put some distance between yourself and the stress? is it something that you have to?....................... \n",
            "\n",
            "Target Sentence : you considered speaking with a therapist? you know why you feel this way? \n",
            "\n",
            "Predicted Sentence : you considered and with a therapist? you know why you feel this way?......................... \n",
            "\n",
            "Target Sentence : times people take laughter as a cue that what doing is okay. do the same thing whenever someone picks on me, even though it hurts inside for all of us. let other opinions influence how you see yourself, as hard as that can be. & lt ; 3 \n",
            "\n",
            "Predicted Sentence : times people take as a that what doing is okay. let other great how you see yourself, as hard as that can be. & ; 3............................ \n",
            "\n",
            "Target Sentence : quite a few things ive noticed for myself. ill over simplify my thought process to get the point across. list them quickly. thought that if could go through physical pain, then can go through mental pain. physical scar is easier to understand that an emotional one. to tend to my wounds to stop the bleeding and cover them up distracted me from what really wanted to tend to and cover up. was a couple times where self harmed because my thoughts were too overwhelming while was trying to fall asleep., would spend some time in the bathroom alone, taking care of my bleeding and by the time it was all finished, forgot what was thinking about. harm was an \n",
            "\n",
            "Predicted Sentence : a few things ive occasionally for myself. over. my thought process to get the point.. them quickly. thought that if could go through. pain, then can go through mental pain.. much is easier to understand that an emotional one. toad to my to stop the you and them up. me from what really. to tend to and up. was a couple times where self because my thoughts were too overwhelming while was trying to fall trick., would spend some time in the bathroom alone, taking care of my you and by the time it was all finished, what was thinking about. harm was an \n",
            "\n",
            "Target Sentence : any chance do you think you're in a loop. food can make you depressed. being depressed probably makes you eat more junk food? a while that was my problem too. if its just random, ill probably binge and feel bad later. to indulge in more later to try to cheer up. \n",
            "\n",
            "Predicted Sentence : any chance do you think you're in a you.. can make you depressed. being depressed probably makes you eat more can '?.................................................... \n",
            "\n",
            "Target Sentence : ' m sorry you feel like this. daughter has tried to kill herself 3 times and pretty much have been mentally preparing myself for the worst for the last 6 months bc she was so so depressed and non responsive to meds, therapy etc. the craziest thing happened... she got involved in an old hobby, met new people she had stuff in common with, and just 5 minutes ago told me that she is going out to lunch with someone from that group. point is... things can change. hope things can change for you soon. \n",
            "\n",
            "Predicted Sentence : ' m sorry you feel like this. point is... things can change. hope things can change for you soon............................................. \n",
            "\n",
            "Target Sentence : is the thing. you open up to these people and they through it in your face right after, then you know they aren't real friends. real friend is willing to sit through peoples problems for hours. once sat and chatted with a friend about his issues for some 7 odd hours hours just talking till 7am, never gave up on the conversation, he did the same for me a while later down the road when was having problems. who aren't willing to talk to you and help you aren't worth keeping around. me there is a friend out there who is willing to listen. guess do keep in mind that not everyone has infinite patients and has their own problems. good luck \n",
            "\n",
            "Predicted Sentence : is the thing. you open up to these people and they through it in your face right after, then you know they aren't real friends. real friend is to sit through problems for hours. who aren't to talk to you and help you aren't worth keeping around. me there is a friend out there who is to. guess do keep in mind that not everyone has and has their own problems. good luck out there man hope things go well for you. isn't to say you can't havezing, people you hang out with, but never get deep...... \n",
            "\n",
            "Target Sentence : buddy you are not alone. spent a day where literally didn't so much as sit up in bed a couple of weeks ago.'t eat or drink anything so didn't need the bathroom which was good.'m not at a point where have any advice to give or anything but maybe you can find some comfort knowing that other people feel the same as you. \n",
            "\n",
            "Predicted Sentence : buddy you are not alone.'m not at a point where have any advice to give or anything but maybe you can find some comfort, that other people feel the same as you........................................................... \n",
            "\n",
            "Target Sentence : too'll help others, plant trees or something. its others happiness that will make me a bit happy too \n",
            "\n",
            "Predicted Sentence : too'll help others, plant or something. its others happiness that will make me a bit happy too..............,.,........................... \n",
            "\n",
            "Target Sentence : ' m kind of the same way. life sucks, see no light in the proverbial tunnel but rather be alive to see how the world turns out than leave my life for nothingness., believe that there's no life after death. you've ever been unconscious you know what'm talking about, one moment you're awake, the next you wake up somewhere else with no sense of time gaining passed. but forever is death to me, there's simply nothing, it just ends completely. rather suffer than cease to exist. \n",
            "\n",
            "Predicted Sentence : , believe that there's no life after death. you've ever been unconscious you know what'm talking about, one moment you're awake, the next you wake up somewhere else with no of time receiving m. but forever is death to me, there's simply nothing, it just completely. rather suffer than seem to exist.............. \n",
            "\n",
            "Target Sentence : feel exately the same and killing me for bad english \n",
            "\n",
            "Predicted Sentence : for bad........................................ \n",
            "\n",
            "Target Sentence : you? are you still here asking for help and gathered the attention of your fellow and they are insisting they want to listen to you, but you keep denying them. put it any other selfish. used to give myself any excuse not to see my friends or family, but it was all debunked by the question they tell you themselves they felt that what can tell, you really need to work on your negative speech patterns. - or - nothing thinking, hopelessness, knee - jerk dismissals ; these are all habits that depressed people make in their speech that perpetuate their own misery. takes conscious effort to recognize our speech patterns as poisonous and slowly change not cares, you the one suffering \n",
            "\n",
            "Predicted Sentence : you? are you still here asking for help and the attention of your and they are they want to to you, but you keep them. put it any other selfish. - or - nothing thinking, hopelessness, - ands ; these are all that depressed people make in their that to with their own misery. takes conscious effort to our as and facts change not cares, you the one suffering. that your is under your control, and that for help can just as easily becomeing out. love you, but now us back involves you an effort not to give your worst think experiences air time. \n",
            "\n",
            "Target Sentence : hate that usually succumb to the crippling overwhelm that often accompanies me trying to deal with the seemingly insurmountable [ logistically, tangible, financial, and medical ] things must deal with...... as the clock ticks..... \n",
            "\n",
            "Predicted Sentence : that often acc. me trying to deal with the seemingly insur to [ly,,, and medical ] things must deal with...... as thes............,....................... think. \n",
            "\n",
            "Target Sentence : ' m sorry to hear how you're feeling right now, and although everything you do at the moment may feel pointless, there will come a time when your purpose will become apparent, and everything will hopefully make sense. can really say right now is to strive on, do the things that make you happy and cherish the people that make you feel loved. wish you a wonderful future, and hope that one day everything will be better. \n",
            "\n",
            "Predicted Sentence : everything you do at the moment may feel pointless, there will come a time when your will become, and everything will pain make sense. can really say right now is to some on, do the things that make you happy and the people that make you feel loved. wish you a wonderful future, and hope that one day everything will be better... \n",
            "\n",
            "Target Sentence : feel you. feel life is too much and rather die because my anxiety makes me feel it's not worth to be alive. then again want to live because'm terrified of death. just hate that fucking feeling, man. are not alone in this tho. \n",
            "\n",
            "Predicted Sentence : are not alone in this 16..................... \n",
            "\n",
            "Target Sentence : don't really think life is he'll but don't understand the weird sadness or fear of death that people have.'ve known a few people who died. wasn't sad at all. know is going to die and one day am going to die. is that sad? \n",
            "\n",
            "Predicted Sentence : don't really think life is he'll but. know is going to die and one day am going to die. is that sad?... t.... '... don......... \n",
            "\n",
            "Target Sentence : more often than not, i end up giving a half truth. or, i guess more accurately, a partial truth. when i'm with friends 9 / 10 they make me feel good and forget about my shit show of a brain. so i answer with good! or feelin'great now! because at the time, thats how i feel. but mostly i leave out how i felt before seeing or talking to them. that may be wrong or decietful, but i just don't like to burden my friends with this. especially those who can't handle my neuroatypicality. \n",
            "\n",
            "Predicted Sentence : ! or feel vulnerable'great now! because at the time, thats how i feel. but you i out how i felt before seeing or talking to them. that may be wrong orie,, but i just don't like to burden my friends with this. especially those who can't handle my familyical don............. \n",
            "\n",
            "Target Sentence : insomnia is usually one of the first indicators of an oncoming episode. truly wish had hypersomnia though, the few times got it, wasn't rested whatsoever but loved those little moments you have when you wake up and you can feel yourself drifting off to sleep. feeling've felt rarely in my 10 year struggle with insomnia alone. \n",
            "\n",
            "Predicted Sentence : ,omnia is usually one of the first of an on '............. \n",
            "\n",
            "Target Sentence : , it is. feel the same way, got hired 7 months ago, and was depressed for 2 years before that. hate to admit this, but some days cry like the little bitch am in the shower and on my drive to work. guess what, haven't missed a single day of work aside from 9 day vacation that took scattered here and there. have seriously one simple trick : you go there no matter what, don't give yourself a choice. more you sit and wonder the harder it gets. thing, my supervisor almost cried today at work and was complaining in a high pitched voice. kinda brightened my day. \n",
            "\n",
            "Predicted Sentence : , it is.. have wrote one simple : you go there no matter what, don't give yourself a choice.. kinda my day......... \n",
            "\n",
            "Target Sentence : what helps me out is music, talking to friends, and my cat. however these are just distractions and they only make me feel better to a small extent. what do you like doing? what do you personally find enjoyable? has your depression taken your interests away from you? \n",
            "\n",
            "Predicted Sentence : what do you like doing? what do you. find distract? has your depression taken your and away from you?...... and.... and.,.. to for, to.. to............................. \n",
            "\n",
            "Target Sentence : there. one on this earth is worthless, even if you feel that way now. gets better, things change, you will change. browsed through your post history and noticed how young you are. age you're in is a horrible hormonic stage. son had the same feelings at that age - range. did too. if life feels overwhelming and lonely now, this will pass eventually. patient. have to go for a while, but will get back to you with how my son learned to cope. in there. big motherly hug to you! \n",
            "\n",
            "Predicted Sentence : there. one on this earth is worthless, even if you feel that way now. gets better, things change, you will change. browsed through your post history and noticed how young you are. if life feels overwhelming and lonely now, this will pass eventually. patient. have to go for a while, but will get back to you with how my son learned to cope. in there. big motherly hug to you!.. \n",
            "\n",
            "Target Sentence : do it. may feel like it now, but it if you commit suicide you will lose the chance to feel better. do u feel bad right now? \n",
            "\n",
            "Predicted Sentence : do it. may feel like it now, but it if you commit suicide you will lose the chance to feel better..................... \n",
            "\n",
            "Target Sentence : ' s some poetic bollocks, right there.'t they have two numbers?'ve seen that someplace. \n",
            "\n",
            "Predicted Sentence : ' s some, right there.'ve that.......................... \n",
            "\n",
            "Target Sentence : understand how you feel. able to talk to someone without worrying about being judge. that person that you can feel comfortable with is something that many of us have a hard time finding. \n",
            "\n",
            "Predicted Sentence : able to talk to someone have the about being judge of to. this. is.... you, to................................................... \n",
            "\n",
            "Target Sentence : ' s hard. if you're the type of person to post in this sub. yourself is really difficult. loving others is easier. do love you. you love others, find, its easier to love yourself. just saying the words and seeing it echoed back makes me feel good. hope helped. \n",
            "\n",
            "Predicted Sentence : ' s. if you're the, of person to post in this sub. yourself is really difficult. loving others is easier. you love others, find, its easier to love yourself......... \n",
            "\n",
            "Target Sentence : yeah : / my parents make me anxious to the point where i wont get food and water because i'm afraid of running into them. glad to be moving out \n",
            "\n",
            "Predicted Sentence : to be to out '.. yourself....... to............................................................ \n",
            "\n",
            "Target Sentence : you tell anyone? thoughts can be dangerous. \n",
            "\n",
            "Predicted Sentence : thoughts be be to.......... \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq3OwArH31Ri"
      },
      "source": [
        "##2b) Seeker post + non-empathetic response as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ31vNRW4REO"
      },
      "source": [
        "#import csv data file into pandas dataframe\n",
        "data_df = pd.read_csv('drive/MyDrive/data.csv', encoding=\"latin1\")\n",
        "\n",
        "#concatenate seeker_post and source. Concatenated sentences are separated by SPLIT token\n",
        "data_df[\"new_source\"] = data_df[\"seeker_post\"] + ' SPLIT ' + data_df[\"source\"]\n",
        "\n",
        "#drop unnecessary columns, rename, reorder\n",
        "data_df = data_df.drop(['seeker_post', 'source'], axis=1)\n",
        "data_df.rename(columns = {'new_source' : 'source'}, inplace = True)\n",
        "data_df = data_df[['source', 'target']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ssPZ4ls4eBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c2a16c-2fb6-4c61-cab2-aa6634adae43"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print(f'Found GPU at: {device_name}')\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU in use:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('using the CPU')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "GPU in use: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLyDHzur4gDS"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "vocab_size = len( BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True).get_vocab() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85PW_5so4jaK"
      },
      "source": [
        "# extra preprocessing steps\n",
        "# prepend CLS and append SEP, truncate, pad\n",
        "\n",
        "#parameters for the dataset and dataloader\n",
        "BATCH_SIZE = 32\n",
        "max_sent_length= 150\n",
        "\n",
        "#################################### Preprocessing function ####################################\n",
        "def preprocessing(df):\n",
        "\n",
        "    #get the source and target sentences\n",
        "    source_sentences = df.source.values\n",
        "    target_sentences = df.target.values\n",
        "\n",
        "    # initialize the tokenizer for tokenize the sentence and obtain the indexed list using bert vocabulary\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
        "    \n",
        "    encoded_source_sentences , encoded_target_sentences = [] , []\n",
        "    # iterate over every source , target sentence and tokenize them\n",
        "    for s_sent , t_sent in zip(source_sentences , target_sentences ):\n",
        "        #source sentence tokenization\n",
        "        s_encoded_sent = tokenizer.encode(\n",
        "                            s_sent,\n",
        "                            add_special_tokens = True,\n",
        "                            truncation=True,\n",
        "                            max_length = max_sent_length\n",
        "                    )\n",
        "        # target sentence tokenization\n",
        "        t_encoded_sent = tokenizer.encode(\n",
        "                            t_sent,\n",
        "                            add_special_tokens = True,\n",
        "                            truncation=True,\n",
        "                            max_length = max_sent_length\n",
        "                    )\n",
        "\n",
        "\n",
        "        encoded_source_sentences.append(s_encoded_sent)\n",
        "        encoded_target_sentences.append(t_encoded_sent)\n",
        "\n",
        "\n",
        "    \n",
        "    return encoded_source_sentences, encoded_target_sentences\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr-bfL6y4puP"
      },
      "source": [
        "#split into train, val and test sets (80-10-10)\n",
        "train_df, val_df, test_df = \\\n",
        "              np.split(data_df.sample(frac=1, random_state=42), \n",
        "                       [int(.8*len(data_df)), int(.9*len(data_df))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I80peIox4tom"
      },
      "source": [
        "# get the encoded source and target sentence\n",
        "train_source_sentences, train_target_sentences = preprocessing(train_df)\n",
        "val_source_sentences, val_target_sentences = preprocessing(val_df)\n",
        "test_source_sentences, test_target_sentences = preprocessing(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW727NnU4vVy"
      },
      "source": [
        "#define the torch dataset class to feed into the model\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    # class initialization with max sequence len , target and source sentence list\n",
        "    def __init__(self, data_list, target_list, max_sent_length=128):\n",
        "        \"\"\"\n",
        "        @param data_list: list of data tokens \n",
        "        @param target_list: list of data targets \n",
        "        \"\"\"\n",
        "        self.data_list = data_list   # source sentence list\n",
        "        self.target_list = target_list  # target sentence list\n",
        "        self.max_sent_length = max_sent_length   # maximum sentence lenght\n",
        "        assert (len(self.data_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "        \n",
        "    # return the source and target sentences\n",
        "    def __getitem__(self, key, max_sent_length=None):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        if max_sent_length is None:\n",
        "            max_sent_length = self.max_sent_length\n",
        "        token_idx = self.data_list[key][:max_sent_length]\n",
        "        label = self.target_list[key]\n",
        "        return [token_idx, label]\n",
        "\n",
        "    # we use this collate_fn to dynamically pad the batch according to the maximum len inside the sentences\n",
        "    def spam_collate_func(self,batch):\n",
        "        \"\"\"\n",
        "        Customized function for DataLoader that dynamically pads the batch so that all \n",
        "        data have the same length\n",
        "        \"\"\" \n",
        "        data_list = [] \n",
        "        target_list = []\n",
        "        mask_list = []\n",
        "\n",
        "        max_batch_seq_len = None # the length of longest sequence in batch\n",
        "                                 # if it is less than self.max_sent_length\n",
        "                                 # else max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        \"\"\"\n",
        "          # Pad the sequences in your data \n",
        "          # if their length is less than max_batch_seq_len\n",
        "          # or trim the sequences that are longer than self.max_sent_length\n",
        "          # return padded data_list and label_list\n",
        "        \"\"\"\n",
        "        \n",
        "        # find the max sequence length from the batch\n",
        "        max_batch_s_len = max(len(datum[0]) for datum in batch)\n",
        "        max_batch_t_len = max(len(datum[1]) for datum in batch)\n",
        "\n",
        "        #get the maximum from source and target lens\n",
        "        max_batch_seq_len = max(  [  max_batch_s_len , max_batch_t_len ] )\n",
        "\n",
        "        if max_batch_seq_len > self.max_sent_length:\n",
        "          max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        # pad each of the texts in batch wit constant 0\n",
        "        for datum in batch:\n",
        "          # padding the source vector\n",
        "          padded_s_vec = np.pad(np.array(datum[0]),\n",
        "                              pad_width = ((0, max_batch_seq_len - len(datum[0]))),\n",
        "                              mode = \"constant\", constant_values = 0)\n",
        "          # padding the target vector\n",
        "          padded_t_vec = np.pad(np.array(datum[1]),\n",
        "                              pad_width = ((0, max_batch_seq_len - len(datum[1]))),\n",
        "                              mode = \"constant\", constant_values = 0)\n",
        "\n",
        "          data_list.append(padded_s_vec)\n",
        "          target_list.append( padded_t_vec )\n",
        "\n",
        "\n",
        "        # convert to torch tensors \n",
        "        data_list = torch.from_numpy(np.array(data_list))\n",
        "        label_list = torch.from_numpy( np.array(target_list)  )\n",
        "\n",
        "        return [data_list, label_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSwoUOYD44Tg"
      },
      "source": [
        "#define the train dataset and the data loader \n",
        "train_dataset = Seq2SeqDataset( train_source_sentences , train_target_sentences , max_sent_length)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# define the validation dataset and the dataloader\n",
        "valid_dataset = Seq2SeqDataset( val_source_sentences , val_target_sentences , train_dataset.max_sent_length)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=False)\n",
        "\n",
        "# define the test dataset and the dataloader\n",
        "test_dataset = Seq2SeqDataset( test_source_sentences , test_target_sentences , train_dataset.max_sent_length)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQdlSuVb4477"
      },
      "source": [
        "#define device cuda or cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vkLyv7A47KE"
      },
      "source": [
        "#encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        # embedding layer \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmM5OJhP4-iA"
      },
      "source": [
        "#attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoUuaWuy5Bui"
      },
      "source": [
        "#decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_XmRRyf5Enm"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt4MPHk95IyB"
      },
      "source": [
        "#hyperparams\n",
        "INPUT_DIM = vocab_size\n",
        "OUTPUT_DIM = vocab_size\n",
        "ENC_EMB_DIM = 512\n",
        "DEC_EMB_DIM = 512\n",
        "ENC_HID_DIM = 256\n",
        "DEC_HID_DIM = 256\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPwoMPUo5NCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1aec87-795e-4603-aa4c-d4cb2b551eb6"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(30522, 512)\n",
              "    (rnn): GRU(512, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
              "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(30522, 512)\n",
              "    (rnn): GRU(1024, 256)\n",
              "    (fc_out): Linear(in_features=1280, out_features=30522, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrlquWWb5P6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc1d427-3f1e-41e9-ee42-34037a6ad73f"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(30522, 512)\n",
              "    (rnn): GRU(512, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
              "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(30522, 512)\n",
              "    (rnn): GRU(1024, 256)\n",
              "    (fc_out): Linear(in_features=1280, out_features=30522, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etN6HJo95STi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d37382-7049-4a01-bbf4-fa44e78b84e5"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 72,848,954 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2UMii_H5VSU"
      },
      "source": [
        "#define optimizer\n",
        "optimizer = optim.Adam(model.parameters() , lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDA011ZM5Xq5"
      },
      "source": [
        "#define loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DamT4GZl5Z-C"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, ( src , trg ) in enumerate(iterator):\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        #move to device\n",
        "        src , trg = src.to(device) , trg.to(device)\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_derGwg5cw1"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src , trg) in enumerate(iterator):\n",
        "\n",
        "            #move to device\n",
        "            src , trg = src.to(device) , trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3mRRgCi5fcD"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ezdOz0l5jBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d182a61f-51be-4479-fb42-567a83edf018"
      },
      "source": [
        "#training loop\n",
        "\n",
        "N_EPOCHS = 100\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Training the model...\")\n",
        "tr = {'loss': [], 'PPL': []}\n",
        "val = {'loss': [], 'PPL': []}\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_loader  , optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader , criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'drive/MyDrive/tut3-model-2c.pt')\n",
        "\n",
        "    # store logs\n",
        "    tr['loss'].append(train_loss)\n",
        "    tr['PPL'].append(math.exp(train_loss))\n",
        "    val['loss'].append(valid_loss)\n",
        "    val['PPL'].append(math.exp(valid_loss))\n",
        "        \n",
        "    if( (epoch+1)%10==0 ):\n",
        "      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Epoch: 10 | Time: 0m 16s\n",
            "\tTrain Loss: 5.724 | Train PPL: 306.080\n",
            "\t Val. Loss: 5.859 |  Val. PPL: 350.420\n",
            "Epoch: 20 | Time: 0m 16s\n",
            "\tTrain Loss: 5.683 | Train PPL: 293.763\n",
            "\t Val. Loss: 5.877 |  Val. PPL: 356.898\n",
            "Epoch: 30 | Time: 0m 16s\n",
            "\tTrain Loss: 5.642 | Train PPL: 282.121\n",
            "\t Val. Loss: 5.889 |  Val. PPL: 361.174\n",
            "Epoch: 40 | Time: 0m 16s\n",
            "\tTrain Loss: 5.605 | Train PPL: 271.744\n",
            "\t Val. Loss: 5.884 |  Val. PPL: 359.187\n",
            "Epoch: 50 | Time: 0m 16s\n",
            "\tTrain Loss: 5.561 | Train PPL: 260.180\n",
            "\t Val. Loss: 5.900 |  Val. PPL: 364.880\n",
            "Epoch: 60 | Time: 0m 16s\n",
            "\tTrain Loss: 5.526 | Train PPL: 251.208\n",
            "\t Val. Loss: 5.909 |  Val. PPL: 368.465\n",
            "Epoch: 70 | Time: 0m 16s\n",
            "\tTrain Loss: 5.470 | Train PPL: 237.418\n",
            "\t Val. Loss: 5.933 |  Val. PPL: 377.277\n",
            "Epoch: 80 | Time: 0m 16s\n",
            "\tTrain Loss: 5.403 | Train PPL: 222.091\n",
            "\t Val. Loss: 5.961 |  Val. PPL: 387.970\n",
            "Epoch: 90 | Time: 0m 16s\n",
            "\tTrain Loss: 5.333 | Train PPL: 207.013\n",
            "\t Val. Loss: 5.993 |  Val. PPL: 400.493\n",
            "Epoch: 100 | Time: 0m 16s\n",
            "\tTrain Loss: 5.267 | Train PPL: 193.900\n",
            "\t Val. Loss: 6.030 |  Val. PPL: 415.574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKlCygoOM_Jp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "96973420-57a4-4dba-b2b8-e6df696e21dd"
      },
      "source": [
        "#plot train and validation PPL logs\n",
        "\n",
        "fig = plt.figure(figsize=(10,7))\n",
        "\n",
        "y1 = tr['PPL']\n",
        "y2 = val['PPL']\n",
        "\n",
        "plt.plot(y1, \"-b\", label=\"Train perplexity\")\n",
        "plt.plot(y2, \"-r\", label=\"Validation perplexity\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.ylim(0, 850)\n",
        "plt.ylabel('Perplexity', fontsize=12)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.title(\"Train and validation perplexity \\n GRU with attention (BERT base embeddings) - Seeker post + non-empathetic response input\", fontsize = 14)\n",
        "\n",
        "fig.savefig(\"2b-ppl.pdf\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cPWoJqW5lnL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdb90fe-ef6a-4ebb-dffe-7d61c2f8aacd"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/tut3-model-2c.pt'))\n",
        "\n",
        "test_loss_2b = evaluate(model, test_loader, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss_2b:.3f} | Test PPL: {math.exp(test_loss_2b):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 5.797 | Test PPL: 329.175 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T98WqE9C5ve6"
      },
      "source": [
        "def generate_sentence(model, iterator):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    pred_trg_pairs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src , trg) in enumerate(iterator):\n",
        "\n",
        "            #move to device\n",
        "            src , trg = src.to(device) , trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[:,1:]\n",
        "            trg = trg[:,1:]\n",
        "\n",
        "            output = output.detach().cpu().numpy()\n",
        "            trg = trg.detach().cpu().numpy()\n",
        "            pred= np.argmax( output , axis = -1 )\n",
        "\n",
        "            pred = tokenizer.batch_decode( pred , skip_special_tokens=True)\n",
        "            trg = tokenizer.batch_decode( trg , skip_special_tokens = True )\n",
        "\n",
        "            for i_pred , i_trg in zip( pred , trg  ):\n",
        "\n",
        "              print( \"Target Sentence : {} \\n\".format( i_trg )  )\n",
        "              print( \"Predicted Sentence : {} \\n\".format( i_pred )  )\n",
        "\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eJY5_-I5wT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb11ddfd-4f54-4a69-b469-7d0d546595a2"
      },
      "source": [
        "generate_sentence(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target Sentence : . must have meant so much to give yourself to a relationship where that loss hurts this bad. crap, that's quite the level of commitment!, that is something to hang your hat on. have that in you, right? did it once before. shit hurts right now but you have it in you!!. cant imagine how happy you will be when you find the next one... maybe one that fits the possibility of a promising relationship?, think you have a good base here. \n",
            "\n",
            "Predicted Sentence :  \n",
            "\n",
            "Target Sentence : feel only bad things, too. can laugh and joke with my buddies, but it just feels hollow and empty on the inside. even come to think, that often times smile and laugh, cause know a normal person would do this and did it once, so to don't look like a freak, do it at the right times.'m never excited these days. can't even remember the last time was. when decided to lose my virginity to a prostitute when was around 30, had some concerns but wasn't excited. and partys, knew couldn't wait for some when was in my early twenties., \n",
            "\n",
            "Predicted Sentence : ....................... \n",
            "\n",
            "Target Sentence : know. how feel. can barely survive now, never make it. \n",
            "\n",
            "Predicted Sentence : . \n",
            "\n",
            "Target Sentence : ' ve come to help you. can't do much. - they're all lies. can understand how you feel because am exactly in your shoes. kill myself'm scared that'll go to hell.'m stuck. can assure you that you are not alone. if you want, you just got to hold on. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : man, you're saying is actually quite common of someone who is good at art. people with a high degree or artistic talent are really good at recognizing good art, but get frustrated because their own art doesn't live up to the crazy high standard they've set for themselves. suggests that it takes 10, 000 hours to gain mastery in something. is a skill just like anything else. think a lot of people get fooled into this either have it or you don't mentality when it comes to creative arts.'s just not the case. think it's more important that you actually have an obsession and something that you want to be good at! people don \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : swear, if people didn't have families, there would be a lot less depression in this world. guess it really depends on the type of family stress. you realistically put some distance between yourself and the stress? is it something that you have to resolve? \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : you considered speaking with a therapist? you know why you feel this way? \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : times people take laughter as a cue that what doing is okay. do the same thing whenever someone picks on me, even though it hurts inside for all of us. let other opinions influence how you see yourself, as hard as that can be. & lt ; 3 \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : quite a few things ive noticed for myself. ill over simplify my thought process to get the point across. list them quickly. thought that if could go through physical pain, then can go through mental pain. physical scar is easier to understand that an emotional one. to tend to my wounds to stop the bleeding and cover them up distracted me from what really wanted to tend to and cover up. was a couple times where self harmed because my thoughts were too overwhelming while was trying to fall asleep., would spend some time in the bathroom alone, taking care of my bleeding and by the time it was all finished, forgot what was thinking about. harm was an \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : any chance do you think you're in a loop. food can make you depressed. being depressed probably makes you eat more junk food? a while that was my problem too. if its just random, ill probably binge and feel bad later. to indulge in more later to try to cheer up. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : ' m sorry you feel like this. daughter has tried to kill herself 3 times and pretty much have been mentally preparing myself for the worst for the last 6 months bc she was so so depressed and non responsive to meds, therapy etc. the craziest thing happened... she got involved in an old hobby, met new people she had stuff in common with, and just 5 minutes ago told me that she is going out to lunch with someone from that group. point is... things can change. hope things can change for you soon. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : is the thing. you open up to these people and they through it in your face right after, then you know they aren't real friends. real friend is willing to sit through peoples problems for hours. once sat and chatted with a friend about his issues for some 7 odd hours hours just talking till 7am, never gave up on the conversation, he did the same for me a while later down the road when was having problems. who aren't willing to talk to you and help you aren't worth keeping around. me there is a friend out there who is willing to listen. guess do keep in mind that not everyone has infinite patients and has their own problems. good luck \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : buddy you are not alone. spent a day where literally didn't so much as sit up in bed a couple of weeks ago.'t eat or drink anything so didn't need the bathroom which was good.'m not at a point where have any advice to give or anything but maybe you can find some comfort knowing that other people feel the same as you. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : too'll help others, plant trees or something. its others happiness that will make me a bit happy too \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : ' m kind of the same way. life sucks, see no light in the proverbial tunnel but rather be alive to see how the world turns out than leave my life for nothingness., believe that there's no life after death. you've ever been unconscious you know what'm talking about, one moment you're awake, the next you wake up somewhere else with no sense of time gaining passed. but forever is death to me, there's simply nothing, it just ends completely. rather suffer than cease to exist. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : feel exately the same and killing me for bad english \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : you? are you still here asking for help and gathered the attention of your fellow and they are insisting they want to listen to you, but you keep denying them. put it any other selfish. used to give myself any excuse not to see my friends or family, but it was all debunked by the question they tell you themselves they felt that what can tell, you really need to work on your negative speech patterns. - or - nothing thinking, hopelessness, knee - jerk dismissals ; these are all habits that depressed people make in their speech that perpetuate their own misery. takes conscious effort to recognize our speech patterns as poisonous and slowly change not cares, you the one suffering \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : hate that usually succumb to the crippling overwhelm that often accompanies me trying to deal with the seemingly insurmountable [ logistically, tangible, financial, and medical ] things must deal with...... as the clock ticks..... \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : ' m sorry to hear how you're feeling right now, and although everything you do at the moment may feel pointless, there will come a time when your purpose will become apparent, and everything will hopefully make sense. can really say right now is to strive on, do the things that make you happy and cherish the people that make you feel loved. wish you a wonderful future, and hope that one day everything will be better. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : feel you. feel life is too much and rather die because my anxiety makes me feel it's not worth to be alive. then again want to live because'm terrified of death. just hate that fucking feeling, man. are not alone in this tho. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : don't really think life is he'll but don't understand the weird sadness or fear of death that people have.'ve known a few people who died. wasn't sad at all. know is going to die and one day am going to die. is that sad? \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : more often than not, i end up giving a half truth. or, i guess more accurately, a partial truth. when i'm with friends 9 / 10 they make me feel good and forget about my shit show of a brain. so i answer with good! or feelin'great now! because at the time, thats how i feel. but mostly i leave out how i felt before seeing or talking to them. that may be wrong or decietful, but i just don't like to burden my friends with this. especially those who can't handle my neuroatypicality. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : insomnia is usually one of the first indicators of an oncoming episode. truly wish had hypersomnia though, the few times got it, wasn't rested whatsoever but loved those little moments you have when you wake up and you can feel yourself drifting off to sleep. feeling've felt rarely in my 10 year struggle with insomnia alone. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : , it is. feel the same way, got hired 7 months ago, and was depressed for 2 years before that. hate to admit this, but some days cry like the little bitch am in the shower and on my drive to work. guess what, haven't missed a single day of work aside from 9 day vacation that took scattered here and there. have seriously one simple trick : you go there no matter what, don't give yourself a choice. more you sit and wonder the harder it gets. thing, my supervisor almost cried today at work and was complaining in a high pitched voice. kinda brightened my day. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : what helps me out is music, talking to friends, and my cat. however these are just distractions and they only make me feel better to a small extent. what do you like doing? what do you personally find enjoyable? has your depression taken your interests away from you? \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : there. one on this earth is worthless, even if you feel that way now. gets better, things change, you will change. browsed through your post history and noticed how young you are. age you're in is a horrible hormonic stage. son had the same feelings at that age - range. did too. if life feels overwhelming and lonely now, this will pass eventually. patient. have to go for a while, but will get back to you with how my son learned to cope. in there. big motherly hug to you! \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : do it. may feel like it now, but it if you commit suicide you will lose the chance to feel better. do u feel bad right now? \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : ' s some poetic bollocks, right there.'t they have two numbers?'ve seen that someplace. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : understand how you feel. able to talk to someone without worrying about being judge. that person that you can feel comfortable with is something that many of us have a hard time finding. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : ' s hard. if you're the type of person to post in this sub. yourself is really difficult. loving others is easier. do love you. you love others, find, its easier to love yourself. just saying the words and seeing it echoed back makes me feel good. hope helped. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : yeah : / my parents make me anxious to the point where i wont get food and water because i'm afraid of running into them. glad to be moving out \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n",
            "Target Sentence : you tell anyone? thoughts can be dangerous. \n",
            "\n",
            "Predicted Sentence : .. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}