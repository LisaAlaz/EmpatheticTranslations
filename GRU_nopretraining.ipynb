{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLDEq9FHp6CA"
      },
      "source": [
        "#1. GRU encoder-decoder with attention (no pre-training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaqO9OOd2mvs"
      },
      "source": [
        "The following architecture is composed of of a GRU encoder and a GRU decoder, with the addition of an attention layer. We use MLP attention computing the energy score $s_i$ with the formula $s_i = a^T tanh (W_d d_t + W_s h_i)$.  Rather than using a vocabulary of pre-trained embeddings, we randomly initialise an embedding layer which is progressively tuned during training. The dimension of the embedding layer is 300. We use a batch size of 32 samples, Adam optimiser with a learning rate of 0.001, and cross-entropy loss (from which we calculate the perplexity scores by taking the exponential). We also implement dropout (value 0.5) to limit overfitting. The models have ~20 million tunable parameters. Both models are trained for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcRoOsHo-Ns1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6f9d52-b6cb-44b6-cf46-d3fda75bf489"
      },
      "source": [
        "#mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7XAe-vCMuUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ff6f2e-0b9f-418b-ff91-c2c6b73c29a9"
      },
      "source": [
        "#import statements\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator, Example, LabelField\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOcwgzSAOCkE"
      },
      "source": [
        "#set seed\n",
        "seed = 42\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8VbjO_5B7xT"
      },
      "source": [
        "#remove limits to read all the text in the dataframe\n",
        "pd.set_option('max_colwidth', None)\n",
        "pd.set_option('max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7DXmXgZ8LbK"
      },
      "source": [
        "##1a) Non-empathetic response as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8nmSGU4PaGP"
      },
      "source": [
        "#import csv data file into pandas dataframe\n",
        "data_df = pd.read_csv('drive/MyDrive/data.csv', encoding=\"latin1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhW573HjSw3U"
      },
      "source": [
        "#add start and end-of-sentence tokens to source and target strings\n",
        "data_df['source'] = 'START ' + data_df['source'].astype(str) + ' END'\n",
        "data_df['target'] = 'START ' + data_df['target'].astype(str) + ' END'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Av_ZCfJW32"
      },
      "source": [
        "#define functions\n",
        "\n",
        "#convert unicode standard to ascii standard\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "#generate a list of all words from the corpus\n",
        "def vocab_gen( data_file ):\n",
        "\n",
        "  text_list=[]\n",
        "  word_list=[]\n",
        "\n",
        "  for idx , text in enumerate( data_file ):\n",
        "\n",
        "    text = unicodeToAscii(text.strip())\n",
        "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
        "    text=nltk.word_tokenize(text)\n",
        "    #text = [word for word in text if not word in set(stopwords.words(\"english\"))]\n",
        "    lemma=nltk.WordNetLemmatizer()\n",
        "    text=[lemma.lemmatize(word) for word in text]\n",
        "    word_list.extend(text)\n",
        "    text=\" \".join(text)\n",
        "    text_list.append(text)\n",
        "\n",
        "  return word_list\n",
        "\n",
        "word_src = vocab_gen( data_df.source )\n",
        "word_trg = vocab_gen( data_df.target )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j0w8RhTKvce"
      },
      "source": [
        "#count occurrences of words and get vocabulary of unique tokens for source and target\n",
        "\n",
        "word_counter_src = Counter(word_src)\n",
        "vocab_src = sorted(word_counter_src  , key=word_counter_src.get, reverse=True)\n",
        "\n",
        "word_counter_trg = Counter(word_trg)\n",
        "vocab_trg = sorted(word_counter_trg  , key=word_counter_trg.get, reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3zPbPGuLGC-"
      },
      "source": [
        "#word to index lookup and viceversa\n",
        "vocab_to_int_src = {word: ii   for ii, word in enumerate(vocab_src, 1)}\n",
        "int_to_vocab_src = {ii : word  for ii , word in enumerate(vocab_src , 1)}\n",
        "\n",
        "vocab_to_int_trg = {word: ii   for ii, word in enumerate(vocab_trg , 1)}\n",
        "int_to_vocab_trg = {ii : word  for ii , word in enumerate(vocab_trg , 1)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4P0fzZuTYna"
      },
      "source": [
        "#add pad token\n",
        "def update_vocab( vocab_to_int , int_to_vocab ):\n",
        "\n",
        "  vocab_to_int.update( {\"<pad>\":0} )\n",
        "  #vocab_to_int.update( {\"start\":1} )\n",
        "  #vocab_to_int.update( {\"end\":2} )\n",
        "\n",
        "  int_to_vocab.update( {0 : \"<pad>\"} )\n",
        "  #int_to_vocab.update( {1 : \"start\"} )\n",
        "  #int_to_vocab.update( {2 : \"end\"} )\n",
        "\n",
        "  return vocab_to_int , int_to_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8B2-8wfLmnz"
      },
      "source": [
        "vocab_to_int_src , int_to_vocab_src = update_vocab( vocab_to_int_src , int_to_vocab_src  )\n",
        "vocab_to_int_trg , int_to_vocab_trg = update_vocab( vocab_to_int_trg , int_to_vocab_trg  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIPe-8o3MuCo"
      },
      "source": [
        "# extra preprocessing steps\n",
        "\n",
        "#parameters for the dataset and dataloader\n",
        "BATCH_SIZE = 32\n",
        "max_sent_length= 150\n",
        "\n",
        "def tokenize( i_text  , vocab ):\n",
        "\n",
        "    text = unicodeToAscii(i_text.strip())\n",
        "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
        "\n",
        "    text=nltk.word_tokenize(text)\n",
        "    #text = [word for word in text if not word in set(stopwords.words(\"english\"))]\n",
        "    lemma=nltk.WordNetLemmatizer()\n",
        "    text=[lemma.lemmatize(word) for word in text]\n",
        "    text=\" \".join(text)\n",
        "\n",
        "    tokenize_sen =[]\n",
        "    for i_word in text.split():\n",
        "      tokenize_sen.append( vocab[ i_word ] )\n",
        "    \n",
        "    return np.array( tokenize_sen )\n",
        "\n",
        "\n",
        "def preprocessing(df):\n",
        "\n",
        "    source_sentences = df.source.values\n",
        "    target_sentences = df.target.values\n",
        "\n",
        "    \n",
        "    \n",
        "    encoded_source_sentences , encoded_target_sentences = [] , []\n",
        "    for s_sent , t_sent in zip(source_sentences , target_sentences ):\n",
        "\n",
        "        s_encoded_sent = tokenize( s_sent , vocab_to_int_src )[: max_sent_length ]\n",
        "        \n",
        "        t_encoded_sent = tokenize( t_sent , vocab_to_int_trg)[: max_sent_length]\n",
        "\n",
        "        encoded_source_sentences.append(s_encoded_sent)\n",
        "        encoded_target_sentences.append(t_encoded_sent)\n",
        "\n",
        "\n",
        "    \n",
        "    return encoded_source_sentences, encoded_target_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRjfzdBOcJgD"
      },
      "source": [
        "#split into train, val and test sets (80-10-10)\n",
        "train_df, val_df, test_df = \\\n",
        "              np.split(data_df.sample(frac=1, random_state=42), \n",
        "                       [int(.8*len(data_df)), int(.9*len(data_df))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_vfulAZNu0N"
      },
      "source": [
        "#preprocessing. Generate representation of sentences as arrays of vocab indexes\n",
        "train_source_sentences, train_target_sentences = preprocessing(train_df)\n",
        "val_source_sentences, val_target_sentences = preprocessing(val_df)\n",
        "test_source_sentences, test_target_sentences = preprocessing(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYblW_HAf1Rs"
      },
      "source": [
        "#define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oohrOit4QI5j"
      },
      "source": [
        "class Seq2SeqDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_list, target_list, max_sent_length=128):\n",
        "        \"\"\"\n",
        "        @param data_list: list of data tokens \n",
        "        @param target_list: list of data targets \n",
        "        \"\"\"\n",
        "        self.data_list = data_list\n",
        "        self.target_list = target_list\n",
        "        self.max_sent_length = max_sent_length\n",
        "        assert (len(self.data_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "        \n",
        "    def __getitem__(self, key, max_sent_length=None):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        if max_sent_length is None:\n",
        "            max_sent_length = self.max_sent_length\n",
        "        token_idx = self.data_list[key][:max_sent_length]\n",
        "        label = self.target_list[key]\n",
        "        return [token_idx, label]\n",
        "\n",
        "    def attention_masks(self , encoded_sentences):\n",
        "      # attention masks, 0 for padding, 1 for actual token\n",
        "      attention_masks = []\n",
        "      for sent in encoded_sentences:\n",
        "          att_mask = [int(token_id > 0 ) for token_id in sent]\n",
        "          attention_masks.append(att_mask)\n",
        "\n",
        "      return attention_masks\n",
        "\n",
        "    def spam_collate_func(self,batch):\n",
        "        \"\"\"\n",
        "        Customized function for DataLoader that dynamically pads the batch so that all \n",
        "        data have the same length\n",
        "        \"\"\" \n",
        "        data_list = [] \n",
        "        target_list = []\n",
        "        mask_list = []\n",
        "\n",
        "        max_batch_seq_len = None # the length of longest sequence in batch\n",
        "                                 # if it is less than self.max_sent_length\n",
        "                                 # else max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        \"\"\"\n",
        "          # Pad the sequences in your data \n",
        "          # if their length is less than max_batch_seq_len\n",
        "          # or trim the sequences that are longer than self.max_sent_length\n",
        "          # return padded data_list and label_list\n",
        "        \"\"\"\n",
        "        \n",
        "        # find the max sequence length from the batch\n",
        "        max_batch_s_len = max(len(datum[0]) for datum in batch)\n",
        "        max_batch_t_len = max(len(datum[1]) for datum in batch)\n",
        "\n",
        "        max_batch_seq_len = max([max_batch_s_len , max_batch_t_len])\n",
        "\n",
        "        if max_batch_seq_len > self.max_sent_length:\n",
        "          max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        # pad each of the texts in batch\n",
        "        for datum in batch:\n",
        "          padded_s_vec = np.pad(np.array(datum[0]),\n",
        "                              pad_width = ((0, max_batch_seq_len - len(datum[0]))),\n",
        "                              mode = \"constant\", constant_values = 0)\n",
        "          \n",
        "          padded_t_vec = np.pad(np.array(datum[1]),\n",
        "                              pad_width = ((0, max_batch_seq_len - len(datum[1]))),\n",
        "                              mode = \"constant\", constant_values = 0)\n",
        "          \n",
        "          #generate the attention mask\n",
        "          #attention_mask = self.attention_masks([padded_vec])\n",
        "\n",
        "          data_list.append(padded_s_vec)\n",
        "          target_list.append( padded_t_vec )\n",
        "          #mask_list.append(attention_mask[0])\n",
        "\n",
        "\n",
        "        #convert to tensors\n",
        "        data_list = torch.from_numpy(np.array(data_list))\n",
        "        label_list = torch.from_numpy(np.array(target_list))\n",
        "        #mask_list = torch.from_numpy(np.array( mask_list))\n",
        "\n",
        "        return [data_list, label_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOIrHMWPQPF1"
      },
      "source": [
        "train_dataset = Seq2SeqDataset( train_source_sentences , train_target_sentences , max_sent_length)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "\n",
        "valid_dataset = Seq2SeqDataset( val_source_sentences , val_target_sentences , train_dataset.max_sent_length)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=False)\n",
        "\n",
        "\n",
        "test_dataset = Seq2SeqDataset( test_source_sentences , test_target_sentences , train_dataset.max_sent_length)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRa8Mc_5QQGy"
      },
      "source": [
        "x , y = next(iter(valid_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixkdWxGK1oNJ"
      },
      "source": [
        "#encoder class \n",
        "#randomly intialises embedding layer\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYpXJ84R12Dq"
      },
      "source": [
        "#attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyeInmhC182H"
      },
      "source": [
        "#decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u4fjg1_cX0X"
      },
      "source": [
        "#model class (encoder + decoder)\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg , teacher_forcing_ratio = 0.5 , test_stage = False):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        if( test_stage ):\n",
        "          outputs = torch.zeros(max_sent_length, batch_size, trg_vocab_size).to(self.device)\n",
        "        else:\n",
        "          trg_len = trg.shape[0]\n",
        "          outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        if( test_stage ):\n",
        "            input = torch.from_numpy( np.array([ vocab_to_int_src[ \"start\" ]  ]) ).view(batch_size).to(device)\n",
        "            for t in range(1, max_sent_length ):\n",
        "            \n",
        "              #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "              #receive output tensor (predictions) and new hidden state\n",
        "              output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "              \n",
        "              #place predictions in a tensor holding predictions for each token\n",
        "              outputs[t] = output\n",
        "              \n",
        "              #decide if we are going to use teacher forcing or not\n",
        "              teacher_force = random.random() < teacher_forcing_ratio\n",
        "              \n",
        "              #get the highest predicted token from our predictions\n",
        "              top1 = output.argmax(1) \n",
        "              \n",
        "              #if teacher forcing, use actual next token as next input\n",
        "              #if not, use predicted token\n",
        "              input = trg[t] if teacher_force else top1\n",
        "\n",
        "              i_word = int_to_vocab_trg[top1.detach().cpu().numpy()[0]]\n",
        "\n",
        "              if(i_word == \"END\" ):\n",
        "                break\n",
        "\n",
        "            return outputs\n",
        "\n",
        "        else:\n",
        "            #first input to the decoder is the <sos> tokens\n",
        "            input = trg[0,:]\n",
        "\n",
        "            for t in range(1, trg_len):\n",
        "                \n",
        "                #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "                #receive output tensor (predictions) and new hidden state\n",
        "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "                \n",
        "                #place predictions in a tensor holding predictions for each token\n",
        "                outputs[t] = output\n",
        "                \n",
        "                #decide if we are going to use teacher forcing or not\n",
        "                teacher_force = random.random() < teacher_forcing_ratio\n",
        "                \n",
        "                #get the highest predicted token from our predictions\n",
        "                top1 = output.argmax(1) \n",
        "                \n",
        "                #if teacher forcing, use actual next token as next input\n",
        "                #if not, use predicted token\n",
        "                input = trg[t] if teacher_force else top1\n",
        "\n",
        "            return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIyNTAZm2DYP"
      },
      "source": [
        "#hyperparams\n",
        "INPUT_DIM = len( vocab_to_int_src )\n",
        "OUTPUT_DIM = len( vocab_to_int_trg )\n",
        "ENC_EMB_DIM = 300\n",
        "DEC_EMB_DIM = 300\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUwOGweo9kqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb19db2-0297-4dad-8fcb-cf828a36993d"
      },
      "source": [
        "#initialise model weights\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(4931, 300)\n",
              "    (rnn): GRU(300, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(5759, 300)\n",
              "    (rnn): GRU(1324, 512)\n",
              "    (fc_out): Linear(in_features=1836, out_features=5759, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3Kk1QET9mmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab54edb-98d7-4499-daf5-9e43d04209ec"
      },
      "source": [
        "#display number of trainable parameters in the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 20,422,315 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbnEIWhz9q43"
      },
      "source": [
        "#define optimizer\n",
        "optimizer = optim.Adam(model.parameters() , lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYc2ILSK9tue"
      },
      "source": [
        "#define loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT8RsjUvbT9q"
      },
      "source": [
        "#train function\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, ( src , trg ) in enumerate(iterator):\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # move to device\n",
        "        src , trg = src.to(device) , trg.to(device)\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        # trg = [trg len, batch size]\n",
        "        # output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy81A_5AbWg9"
      },
      "source": [
        "#evaluation\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src , trg) in enumerate(iterator):\n",
        "\n",
        "            #move to device\n",
        "            src , trg = src.to(device) , trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNZ9OHiuAASv"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCcN2oWnAGPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f016133-454b-4f00-a4de-3fa7155a33b8"
      },
      "source": [
        "#training loop\n",
        "\n",
        "N_EPOCHS = 100\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Training the model...\")\n",
        "tr = {'loss': [], 'PPL': []}\n",
        "val = {'loss': [], 'PPL': []}\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_loader , optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader , criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'drive/MyDrive/tut3-model-1a.pt')\n",
        "\n",
        "    # store logs\n",
        "    tr['loss'].append(train_loss)\n",
        "    tr['PPL'].append(math.exp(train_loss))\n",
        "    val['loss'].append(valid_loss)\n",
        "    val['PPL'].append(math.exp(valid_loss))\n",
        "    \n",
        "    if( (epoch+1)%5 == 0 ):\n",
        "      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Epoch: 05 | Time: 0m 10s\n",
            "\tTrain Loss: 6.047 | Train PPL: 422.730\n",
            "\t Val. Loss: 6.123 |  Val. PPL: 456.221\n",
            "Epoch: 10 | Time: 0m 10s\n",
            "\tTrain Loss: 6.002 | Train PPL: 404.422\n",
            "\t Val. Loss: 6.126 |  Val. PPL: 457.520\n",
            "Epoch: 15 | Time: 0m 10s\n",
            "\tTrain Loss: 5.968 | Train PPL: 390.867\n",
            "\t Val. Loss: 6.120 |  Val. PPL: 454.918\n",
            "Epoch: 20 | Time: 0m 10s\n",
            "\tTrain Loss: 5.947 | Train PPL: 382.435\n",
            "\t Val. Loss: 6.124 |  Val. PPL: 456.461\n",
            "Epoch: 25 | Time: 0m 10s\n",
            "\tTrain Loss: 5.908 | Train PPL: 367.964\n",
            "\t Val. Loss: 6.085 |  Val. PPL: 439.128\n",
            "Epoch: 30 | Time: 0m 10s\n",
            "\tTrain Loss: 5.856 | Train PPL: 349.348\n",
            "\t Val. Loss: 6.046 |  Val. PPL: 422.367\n",
            "Epoch: 35 | Time: 0m 10s\n",
            "\tTrain Loss: 5.797 | Train PPL: 329.210\n",
            "\t Val. Loss: 6.040 |  Val. PPL: 419.792\n",
            "Epoch: 40 | Time: 0m 10s\n",
            "\tTrain Loss: 5.742 | Train PPL: 311.603\n",
            "\t Val. Loss: 6.028 |  Val. PPL: 414.834\n",
            "Epoch: 45 | Time: 0m 10s\n",
            "\tTrain Loss: 5.647 | Train PPL: 283.300\n",
            "\t Val. Loss: 6.020 |  Val. PPL: 411.415\n",
            "Epoch: 50 | Time: 0m 10s\n",
            "\tTrain Loss: 5.542 | Train PPL: 255.215\n",
            "\t Val. Loss: 5.942 |  Val. PPL: 380.685\n",
            "Epoch: 55 | Time: 0m 10s\n",
            "\tTrain Loss: 5.308 | Train PPL: 201.896\n",
            "\t Val. Loss: 5.850 |  Val. PPL: 347.145\n",
            "Epoch: 60 | Time: 0m 10s\n",
            "\tTrain Loss: 5.052 | Train PPL: 156.319\n",
            "\t Val. Loss: 5.621 |  Val. PPL: 276.066\n",
            "Epoch: 65 | Time: 0m 10s\n",
            "\tTrain Loss: 4.873 | Train PPL: 130.712\n",
            "\t Val. Loss: 5.387 |  Val. PPL: 218.491\n",
            "Epoch: 70 | Time: 0m 10s\n",
            "\tTrain Loss: 4.459 | Train PPL:  86.394\n",
            "\t Val. Loss: 5.232 |  Val. PPL: 187.227\n",
            "Epoch: 75 | Time: 0m 10s\n",
            "\tTrain Loss: 4.329 | Train PPL:  75.859\n",
            "\t Val. Loss: 5.200 |  Val. PPL: 181.357\n",
            "Epoch: 80 | Time: 0m 10s\n",
            "\tTrain Loss: 4.100 | Train PPL:  60.358\n",
            "\t Val. Loss: 5.150 |  Val. PPL: 172.482\n",
            "Epoch: 85 | Time: 0m 10s\n",
            "\tTrain Loss: 3.911 | Train PPL:  49.933\n",
            "\t Val. Loss: 4.991 |  Val. PPL: 147.141\n",
            "Epoch: 90 | Time: 0m 10s\n",
            "\tTrain Loss: 3.745 | Train PPL:  42.325\n",
            "\t Val. Loss: 4.970 |  Val. PPL: 144.096\n",
            "Epoch: 95 | Time: 0m 10s\n",
            "\tTrain Loss: 3.580 | Train PPL:  35.865\n",
            "\t Val. Loss: 5.016 |  Val. PPL: 150.745\n",
            "Epoch: 100 | Time: 0m 10s\n",
            "\tTrain Loss: 3.611 | Train PPL:  37.018\n",
            "\t Val. Loss: 5.026 |  Val. PPL: 152.271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ps576JSxu9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "853df439-9170-4b0e-a613-131b1e6c6c32"
      },
      "source": [
        "#plot PPL curves for train and validation set\n",
        "\n",
        "fig = plt.figure(figsize=(10,7))\n",
        "\n",
        "\n",
        "y1 = tr['PPL']\n",
        "y2 = val['PPL']\n",
        "\n",
        "plt.plot(y1, \"-b\", label=\"Train perplexity\")\n",
        "plt.plot(y2, \"-r\", label=\"Validation perplexity\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.ylim(0, 900)\n",
        "plt.ylabel('Perplexity', fontsize=12)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.title(\"Train and validation perplexity \\n GRU with attention (no pre-training) - Non-empathetic response input\", fontsize = 14)\n",
        "\n",
        "fig.savefig(\"1a-ppl.pdf\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaesdjdRmzK4",
        "outputId": "80c153fc-a364-49d1-b550-8f99a4c50091"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/tut3-model-1a.pt'))\n",
        "\n",
        "test_loss_1a = evaluate(model, test_loader , criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss_1a:.3f} | Test PPL: {math.exp(test_loss_1a):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 5.484 | Test PPL: 240.774 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9RKA5SGwFQu"
      },
      "source": [
        "#define functions to do a final visual evaluation on a subset of the test set\n",
        "\n",
        "def int_2_words( i_text , vocab ) :\n",
        "\n",
        "  text_list = []\n",
        "  for i_idx in i_text :\n",
        "    i_word =  vocab[i_idx] \n",
        "\n",
        "    if(i_word=='<pad>'):\n",
        "      continue\n",
        "\n",
        "    text_list.append( i_word )\n",
        "\n",
        "    if(i_word==\"END\"):\n",
        "      break\n",
        "\n",
        "  text=\" \".join(text_list)\n",
        "\n",
        "  return text\n",
        "  \n",
        "\n",
        "def generate_sentence(model, iterator):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    pred_trg_pairs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src , trg) in enumerate(iterator):\n",
        "\n",
        "            #move to device\n",
        "            src , trg = src.to(device) , trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[:,1:]\n",
        "            trg = trg[:,1:]\n",
        "\n",
        "            output = output.detach().cpu().numpy()\n",
        "            trg = trg.detach().cpu().numpy()\n",
        "            pred= np.argmax( output , axis = -1 )\n",
        "\n",
        "            for i_pred , i_trg in zip( pred , trg  ):\n",
        "\n",
        "              trg_sen = int_2_words( i_trg , int_to_vocab_trg  )\n",
        "              pred_sen = int_2_words( i_pred , int_to_vocab_trg )\n",
        "\n",
        "              print( \"Target Sentence : {} \\n\".format( trg_sen )  )\n",
        "              print( \"Predicted Sentence : {} \\n\".format( pred_sen )  )\n",
        "\n",
        "            break\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwoJZ6-KQIIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab3e4d6-1578-444a-f05c-40bee324b9b6"
      },
      "source": [
        "#test model on test set\n",
        "generate_sentence(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target Sentence : Wow . It must have meant so much to give yourself to a relationship where that loss hurt this bad . Holy crap that s quite the level of commitment ! Well that is something to hang your hat on . You have that in you right ? You did it once before . Yea shit hurt right now but you have it in you ! ! .I cant imagine how happy you will be when you find the next one . . .maybe one that fit the possibility of a promising relationship ? Well I think you have a good base here . END \n",
            "\n",
            "Predicted Sentence :  \n",
            "\n",
            "Target Sentence : I feel only bad thing too . I can laugh and joke with my buddy but it just feel hollow and empty on the inside . I even come to think that I often time smile and laugh cause I know a normal person would do this and I did it once so to don t look like a freak I do it at the right time . But I m never excited these day . I can t even remember the last time I wa . Even when I decided to lose my virginity to a prostitute when I wa around I had some concern but I wasn t excited . Birthdays and party I knew I couldn t wait for some when I wa in my early twenty . Nowadays I actually have to push myself to go out and meet friend even though I absolutly KNOW \n",
            "\n",
            "Predicted Sentence : Sometimes person would do this and I did it once so to don t look like a I I do it at the right time . But I m never excited these day . I can t even remember the last time I wa . Even when I decided to lose my . to a . when I wa around I had some u but I wasn t excited . I and party I knew I couldn t wait for some when I wa in my . . . . I actually have to push myself to go out and everything friend even though I . would it will be a nice evening . So . I can t give you any advice . END \n",
            "\n",
            "Target Sentence : I know . Exactly how I feel . If I can barely survive now never make it . END \n",
            "\n",
            "Predicted Sentence : I know . I . I I . I . . . I . . . . . I I . . . . . . . I . . . . . . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : I ve come to help you . But I can t do much . Encouragement they re all lie . I can understand how you feel because I am exactly in your shoe . If I kill myself I m scared that I ll go to hell . So I m stuck . But I can assure you that you are not alone . And if you want you just got to hold on . END \n",
            "\n",
            "Predicted Sentence : I ve come to help you . But I can . you that you are not alone . And if you want you just got to known on . I . . I . I I . . I . . I . . . I . I I . . . . . . . . . . . . . . . . I . . . . . . . I . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : Nah man What you re saying is actually quite common of someone who is good at art . Most people with a high degree or artistic talent are really good at recognizing good art but get frustrated because their own art doesn t live up to the crazy high standard they ve set for themselves . Malcom Gladwell suggests that it take hour to gain mastery in something . Art is a skill just like anything else . I think a lot of people get fooled into this You either have it or you don t mentality when it come to creative art . It s just not the case . I think it s more important that you actually have an obsession and something that you want to be good at ! Most people don t have that . Just keep practicing and never give up . If \n",
            "\n",
            "Predicted Sentence : I man What you re saying is actually quite common of someone who is good at rich . You either have it or you don t . when it come to creative art . It s just not the case . I think it s more important that you actually have an . and something that you want to be good at ! Most people don t have that . Just keep practicing and never give up . If you have the drive you will succeed eventually . You ll get better and better every day . END \n",
            "\n",
            "Target Sentence : I swear if people didn t have family there would be a lot le depression in this world . I guess it really depends on the type of family stress . Can you realistically put some distance between yourself and the stress ? Or is it something that you have to resolve ? END \n",
            "\n",
            "Predicted Sentence : I blip if people didn t have family there would be a lot le depression in this world . I guess it really to on the type of family stress . Can you . put some distance between yourself and the stress ? Or is it something that you have to . ? END \n",
            "\n",
            "Target Sentence : Have you considered speaking with a therapist ? Do you know why you feel this way ? END \n",
            "\n",
            "Predicted Sentence : Have you considered into with a therapist ? Do you know why you feel this way ? END \n",
            "\n",
            "Target Sentence : Often time people take laughter a a cue that what doing is okay . I do the same thing whenever someone pick on me even though it hurt inside for all of u . let other opinion influence how you see yourself a hard a that can be . lt END \n",
            "\n",
            "Predicted Sentence : I time people take . a a . that what doing is constructive . let other opinion something how you see yourself a hard a that can be . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : quite a few thing ive noticed for myself . ill over simplify my thought process to get the point across . Ill list them quickly . I thought that if I could go through physical pain then I can go through mental pain . A physical scar is easier to understand that an emotional one . Having to tend to my wound to stop the bleeding and cover them up distracted me from what I really wanted to tend to and cover up . There wa a couple time where I self harmed because my thought were too overwhelming while I wa trying to fall asleep . Afterwards I would spend some time in the bathroom alone taking care of my bleeding and by the time it wa all finished I forgot what I wa thinking about . Self harm wa an effort to try and beat life at \n",
            "\n",
            "Predicted Sentence : quite a few thing ive occasionally for myself . ill over I my thought process to get the point . . Ill list them quickly . I thought that if I could go through I pain then I can go through mental pain . A I mad is easier to understand that an emotional one . great to subreddit to my the to stop the you and I them up . me from what I really felt to subreddit to and I up . There wa a couple time where I self . because my thought were too overwhelming while I wa trying to fall and . . I would happiness some time in the bathroom alone taking care of my you and by the time it wa all finished I . what I wa thinking about . Self harm wa an effort to try and awful life at \n",
            "\n",
            "Target Sentence : By any chance do you think you re in a loop . Junk food can make you depressed . And being depressed probably make you eat more junk food ? For a while that wa my problem too . Even if it just random ill probably binge and feel bad later . Only to indulge in more later to try to cheer up . END \n",
            "\n",
            "Predicted Sentence : By any chance do you think you re in a just . By . can make you depressed . And being depressed probably make you eat more make . ? I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : I m sorry you feel like this . My daughter ha tried to kill herself time and I pretty much have been mentally preparing myself for the worst for the last month bc she wa so so depressed and non responsive to med therapy etc . Then the craziest thing happened . . .she got involved in an old hobby met new people she had stuff in common with and just minute ago told me that she is going out to lunch with someone from that group . My point is . . .things can change . I hope thing can change for you soon . END \n",
            "\n",
            "Predicted Sentence : I m sorry you feel like this . My point is . . that can change . I hope thing can change for you soon . I . . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : Here is the thing . If you open up to these people and they through it in your face right after then you know they aren t real friend . A real friend is willing to sit through people problem for hour . I once sat and chatted with a friend about his issue for some odd hour hour just talking till am never gave up on the conversation he did the same for me a while later down the road when I wa having problem . People who aren t willing to talk to you and help you aren t worth keeping around . Believe me there is a friend out there who is willing to listen . I guess do keep in mind that not everyone ha infinite patient and ha their own problem . good luck out there man hope thing go well for you . \n",
            "\n",
            "Predicted Sentence : Here is the thing . If you open up to these people and they through it in your multiple right after then you know they aren t real friend . A real friend is D to sit through people problem for hour . People who aren t attachment to talk to you and help you aren t worth keeping around . . me there is a friend out there who is attachment to listen . I guess do keep in mind that not everyone ha . patient and ha their own problem . good luck out there man hope thing go well for you . This isn t to say you can t have acquaintance people you hang out with but never get deep END \n",
            "\n",
            "Target Sentence : Hey buddy you are not alone . I spent a day where I literally didn t so much a sit up in bed a couple of week ago . Didn t eat or drink anything so I didn t need the bathroom which wa good . I m not at a point where I have any advice to give or anything but maybe you can find some comfort knowing that other people feel the same a you . END \n",
            "\n",
            "Predicted Sentence : Hey buddy you are arriving alone . I m not at a point where I have any advice to give or anything but maybe you can find some comfort I that other people feel the same a you . END \n",
            "\n",
            "Target Sentence : Me too I ll help others plant tree or something .Perhaps it others happiness that will make me a bit happy too END \n",
            "\n",
            "Predicted Sentence : Me too I ll help others plant or or something . it others happiness that will make me a bit happy too I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : I m kind of the same way . My life suck I see no light in the proverbial tunnel but I rather be alive to see how the world turn out than leave my life for nothingness . Disclaimer I believe that there s no life after death . If you ve ever been unconscious you know what I m talking about one moment you re awake the next you wake up somewhere else with no sense of time gaining passed . That but forever is death to me there s simply nothing it just end completely . I rather suffer than cease to exist . END \n",
            "\n",
            "Predicted Sentence : I I believe that there s no life after death . If you ve ever been unconscious you know what I m talking about one moment you re awake the next you wake up somewhere else with no sense of time gaining so . That but forever is death to me there s confident nothing it just end completely . I rather suffer than to to exist . END \n",
            "\n",
            "Target Sentence : I feel exately the same and killing me Sorry for bad english END \n",
            "\n",
            "Predicted Sentence : Sorry for bad I I . . . . . . I I . . . . . I I . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : Do you ? Or are you still here asking for help and gathered the attention of your fellow Redditors and they are insisting they want to listen to you but you keep denying them . I put it any other selfish . I used to give myself any excuse not to see my friend or family but it wa all debunked by the question they tell you themselves they felt that what I can tell you really need to work on your negative speech pattern . All or nothing thinking hopelessness knee jerk dismissal these are all habit that depressed people make in their speech that perpetuate their own misery . It take conscious effort to recognize our speech pattern a poisonous and slowly change not care you the one suffering . Recognize that your speech is under your control and that cry for help can just a easily \n",
            "\n",
            "Predicted Sentence : Do you ? Or are you still here antidepressant for help and . the return of your . I and they are . they want to listen to you but you keep . them . I put it any other man . All or nothing thinking hopelessness . my . these are all habit that depressed people make in their . that . their own misery . It take impersonal effort to is our . have a . and slowly change not care you the one suffering . . that your . is under your control and that cry for help can just a memory become . out . We love you but loving u back involves making an effort not to give your worst . air time . END \n",
            "\n",
            "Target Sentence : I hate that I usually succumb to the crippling overwhelm that often accompanies me trying to deal with the seemingly insurmountable logistically tangible financial and medical thing I must deal with . . . . . . a the clock tick . . . . . END \n",
            "\n",
            "Predicted Sentence : I that often . me trying to deal with the seemingly I I . I and medical thing I at deal with . . . . . . a the I . . . . . . I . . . . . . . . . . . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : I m sorry to hear how you re feeling right now and although everything you do at the moment may feel pointless there will come a time when your purpose will become apparent and everything will hopefully make sense . All I can really say right now is to strive on do the thing that make you happy and cherish the people that make you feel loved . I wish you a wonderful future OP and hope that one day everything will be better . END \n",
            "\n",
            "Predicted Sentence : and decision everything you do at the moment He feel how there will come a time when your Viktor will become . and everything will We make sense . All I can really say right now is to some on do the thing that make you happy and . the people that make I feel loved . I wish you a wonderful future OP and hope that one day everything will be better . END \n",
            "\n",
            "Target Sentence : I feel you . I feel life is too much and I rather die because my anxiety make me feel it s not worth to be alive . But then again I want to live because I m terrified of death . I just hate that fucking feeling man . You are not alone in this tho . END \n",
            "\n",
            "Predicted Sentence : You are not alone in this wa . I . . I I . I . to . I . . I I . . . . I . I I . I . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : I don t really think life is he ll but I don t understand the weird sadness or fear of death that people have . I ve known a few people who died . I wasn t sad at all . Everyone I know is going to die and one day I am going to die . Why is that sad ? END \n",
            "\n",
            "Predicted Sentence : I don t really think life is he ll but . Everyone I know is going to die and one day I am going to die . Why is that sad ? I . . . . . . . . . . . . . . . . . . . . you . . . . . you . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : more often than not i end up giving a half truth . or i guess more accurately a partial truth . when i m with friend they make me feel good and forget about my shit show of a brain . so i answer with good ! or feelin great now ! because at the time thats how i feel . but mostly i leave out how i felt before seeing or talking to them . that may be wrong or decietful but i just don t like to burden my friend with this . especially those who can t handle my neuroatypicality . END \n",
            "\n",
            "Predicted Sentence : good ! or . great now ! because at the time thats how i feel . but mostly i leave out how i felt before seeing or talking to them . that may be wrong or . but i just don t like to burden my friend with this . especially those who . t handle my . . END \n",
            "\n",
            "Target Sentence : Bad insomnia is usually one of the first indicator of an oncoming episode . I truly wish I had hypersomnia though the few time I got it I wasn t rested whatsoever but I loved those little moment you have when you wake up and you can feel yourself drifting off to sleep . A feeling I ve felt rarely in my year struggle with insomnia alone . END \n",
            "\n",
            "Predicted Sentence : Bad insomnia is usually one of the first . of an I episode . I . . . I . . a . . to . . I . . . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : Aye it is . I feel the same way I got hired month ago and wa depressed for year before that . I hate to admit this but some day I cry like the little bitch I am in the shower and on my drive to work . But guess what I haven t missed a single day of work aside from day vacation that I took scattered here and there . I have seriously one simple trick you go there no matter what don t give yourself a choice . The more you sit and wonder the harder it get . Funny thing my supervisor almost cried today at work and wa complaining in a high pitched voice xD . It kinda brightened my day LOL . END \n",
            "\n",
            "Predicted Sentence : I it is . . I have wrote one simple you you go there no I what don t give yourself a choice . . It classic I my day I . I . . . . . . . . . . . . . . . . . . . . can . . . . . END \n",
            "\n",
            "Target Sentence : what help me out is music talking to friend and my cat . however these are just distraction and they only make me feel better to a small extent . what do you like doing ? what do you personally find enjoyable ? ha your depression taken your interest away from you ? END \n",
            "\n",
            "Predicted Sentence : what do you like doing ? what do you die find enjoyable ? ha your depression taken your wa away from you ? I . . . I I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "Target Sentence : Hi there . No one on this earth is worthless even if you feel that way now . It get better thing change you will change . I browsed through your post history and noticed how young you are . The age you re in is a horrible hormonic stage . My son had the same feeling at that age range . I did too . Even if life feel overwhelming and lonely now this will pas eventually . Be patient . I have to go for a while but will get back to you with how my son learned to cope . Hang in there . A big motherly hug to you ! END \n",
            "\n",
            "Predicted Sentence : I there . No one on this earth is worthless even if you feel that way now . It get better thing change you will change . I browsed through your post history and noticed how young you are . Even if life feel overwhelming and lonely now this will pas eventually . . patient . I have to . for a while but will get back to you with how my son learned to cope . is . . . A big . hug to you ! END \n",
            "\n",
            "Target Sentence : Dont do it . You may feel like it now but it if you commit suicide you will lose the chance to feel better . Why do u feel bad right now ? END \n",
            "\n",
            "Predicted Sentence : what do to . You may feel like it now but it if you commit suicide you will lose the chance to feel better . END \n",
            "\n",
            "Target Sentence : That s some poetic bollock right there . Don t they have two number ? I ve seen that someplace . END \n",
            "\n",
            "Predicted Sentence : Hi s some . . right there . I ve I that I . I . I . I I . . . . I END \n",
            "\n",
            "Target Sentence : I understand how you feel . Being able to talk to someone without worrying about being judge . Knowing that person that you can feel comfortable with is something that many of u have a hard time finding . END \n",
            "\n",
            "Predicted Sentence : Dont able to talk to someone without getting about being do I to . I . . . . . . I . . I now . I easier . . you I . I . . easier . . . . END \n",
            "\n",
            "Target Sentence : That s hard . Especially if you re the type of person to post in this sub . Loving yourself is really difficult . But loving others is easier . And I do love you . When you love others I find it easier to love yourself . Even just saying the word and seeing it echoed back make me feel good . I hope I helped . END \n",
            "\n",
            "Predicted Sentence : That s hard . Especially if you re the type of person to post in this haha . I yourself is really difficult . But loving others is easier . When you love others I find it easier to love yourself . END \n",
            "\n",
            "Target Sentence : yeah my parent make me anxious to the point where i wont get food and water because i m afraid of running into them . glad to be moving out END \n",
            "\n",
            "Predicted Sentence : Being to be moving statement I I . . I I I I . I . END \n",
            "\n",
            "Target Sentence : Can you tell anyone ? Bad thought can be dangerous . END \n",
            "\n",
            "Predicted Sentence : That thought can be dangerous . I . I I I I I I I . I . I I . I I I I I I I I . . . I . I . . . . I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp1-y1vI-60b"
      },
      "source": [
        "##1b) Seeker post + non-empathetic response as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQE5aMPXCGBI"
      },
      "source": [
        "#re-import csv data file into pandas dataframe\n",
        "data_df = pd.read_csv('drive/MyDrive/data.csv', encoding=\"latin1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE5wpNUn_Alf"
      },
      "source": [
        "#concatenate seeker_post and source. Concatenated sentences are separated by a SPLIT token\n",
        "data_df[\"new_source\"] = data_df[\"seeker_post\"] + ' SPLIT ' + data_df[\"source\"]\n",
        "\n",
        "#drop unnecessary columns, rename, reorder\n",
        "data_df = data_df.drop(['seeker_post', 'source'], axis=1)\n",
        "data_df.rename(columns = {'new_source' : 'source'}, inplace = True)\n",
        "data_df = data_df[['source', 'target']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kmpun_ZDxFF"
      },
      "source": [
        "#add start and end-of-sentence tokens\n",
        "data_df['source'] = 'START ' + data_df['source'].astype(str) + ' END'\n",
        "data_df['target'] = 'START ' + data_df['target'].astype(str) + ' END'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noYlY9jaD1kT"
      },
      "source": [
        "#define functions\n",
        "\n",
        "#convert unicode standard to ascii standard\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "#generate a word list from the corpus\n",
        "def vocab_gen( data_file ):\n",
        "\n",
        "  text_list=[]\n",
        "  word_list=[]\n",
        "\n",
        "  for idx , text in enumerate( data_file ):\n",
        "\n",
        "    text = unicodeToAscii(text.strip())\n",
        "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
        "    text=nltk.word_tokenize(text)\n",
        "    #text = [word for word in text if not word in set(stopwords.words(\"english\"))]\n",
        "    lemma=nltk.WordNetLemmatizer()\n",
        "    text=[lemma.lemmatize(word) for word in text]\n",
        "    word_list.extend(text)\n",
        "    text=\" \".join(text)\n",
        "    text_list.append(text)\n",
        "\n",
        "  return word_list\n",
        "\n",
        "word_src = vocab_gen( data_df.source )\n",
        "word_trg = vocab_gen( data_df.target )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkEPZ2DtD932"
      },
      "source": [
        "#count words and make vocab of unique tokens\n",
        "word_counter_src = Counter(word_src)\n",
        "vocab_src = sorted(word_counter_src  , key=word_counter_src.get, reverse=True)\n",
        "\n",
        "word_counter_trg = Counter(word_trg)\n",
        "vocab_trg = sorted(word_counter_trg  , key=word_counter_trg.get, reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5Zp_JVNEBJh"
      },
      "source": [
        "#word to index and viceversa\n",
        "vocab_to_int_src = {word: ii   for ii, word in enumerate(vocab_src, 1)}\n",
        "int_to_vocab_src = {ii : word  for ii , word in enumerate(vocab_src , 1)}\n",
        "\n",
        "vocab_to_int_trg = {word: ii   for ii, word in enumerate(vocab_trg , 1)}\n",
        "int_to_vocab_trg = {ii : word  for ii , word in enumerate(vocab_trg , 1)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8QT3U9xEFhO"
      },
      "source": [
        "#add pad token to vocab\n",
        "def update_vocab( vocab_to_int , int_to_vocab ):\n",
        "\n",
        "  vocab_to_int.update( {\"<pad>\":0} )\n",
        "  #vocab_to_int.update( {\"start\":1} )\n",
        "  #vocab_to_int.update( {\"end\":2} )\n",
        "\n",
        "  int_to_vocab.update( {0 : \"<pad>\"} )\n",
        "  #int_to_vocab.update( {1 : \"start\"} )\n",
        "  #int_to_vocab.update( {2 : \"end\"} )\n",
        "\n",
        "  return vocab_to_int , int_to_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLUu2Cs_EL8X"
      },
      "source": [
        "vocab_to_int_src , int_to_vocab_src = update_vocab( vocab_to_int_src , int_to_vocab_src  )\n",
        "vocab_to_int_trg , int_to_vocab_trg = update_vocab( vocab_to_int_trg , int_to_vocab_trg  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCZxusDPERi-"
      },
      "source": [
        "# extra preprocessing steps\n",
        "\n",
        "#parameters for the dataset and dataloader\n",
        "BATCH_SIZE = 32\n",
        "max_sent_length= 150\n",
        "\n",
        "def tokenize( i_text  , vocab ):\n",
        "\n",
        "    text = unicodeToAscii(i_text.strip())\n",
        "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
        "\n",
        "    text=nltk.word_tokenize(text)\n",
        "    #text = [word for word in text if not word in set(stopwords.words(\"english\"))]\n",
        "    lemma=nltk.WordNetLemmatizer()\n",
        "    text=[lemma.lemmatize(word) for word in text]\n",
        "    text=\" \".join(text)\n",
        "\n",
        "    tokenize_sen =[]\n",
        "    for i_word in text.split():\n",
        "      tokenize_sen.append( vocab[ i_word ] )\n",
        "    \n",
        "    return np.array( tokenize_sen )\n",
        "\n",
        "\n",
        "def preprocessing(df):\n",
        "\n",
        "    source_sentences = df.source.values\n",
        "    target_sentences = df.target.values\n",
        "\n",
        "    \n",
        "    \n",
        "    encoded_source_sentences , encoded_target_sentences = [] , []\n",
        "    for s_sent , t_sent in zip(source_sentences , target_sentences ):\n",
        "\n",
        "        s_encoded_sent = tokenize( s_sent , vocab_to_int_src )[: max_sent_length ]\n",
        "        \n",
        "        t_encoded_sent = tokenize( t_sent , vocab_to_int_trg)[: max_sent_length]\n",
        "\n",
        "        encoded_source_sentences.append(s_encoded_sent)\n",
        "        encoded_target_sentences.append(t_encoded_sent)\n",
        "\n",
        "\n",
        "    \n",
        "    return encoded_source_sentences, encoded_target_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Jway-CEeRL"
      },
      "source": [
        "#split into train, val and test sets (80-10-10)\n",
        "train_df, val_df, test_df = \\\n",
        "              np.split(data_df.sample(frac=1, random_state=42), \n",
        "                       [int(.8*len(data_df)), int(.9*len(data_df))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnumeGkyEj_3"
      },
      "source": [
        "#preprocessing\n",
        "train_source_sentences, train_target_sentences = preprocessing(train_df)\n",
        "val_source_sentences, val_target_sentences = preprocessing(val_df)\n",
        "test_source_sentences, test_target_sentences = preprocessing(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwb9w-vUEoAH"
      },
      "source": [
        "#define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aaopSViEo_r"
      },
      "source": [
        "class Seq2SeqDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_list, target_list, max_sent_length=128):\n",
        "        \"\"\"\n",
        "        @param data_list: list of data tokens \n",
        "        @param target_list: list of data targets \n",
        "        \"\"\"\n",
        "        self.data_list = data_list\n",
        "        self.target_list = target_list\n",
        "        self.max_sent_length = max_sent_length\n",
        "        assert (len(self.data_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "        \n",
        "    def __getitem__(self, key, max_sent_length=None):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        if max_sent_length is None:\n",
        "            max_sent_length = self.max_sent_length\n",
        "        token_idx = self.data_list[key][:max_sent_length]\n",
        "        label = self.target_list[key]\n",
        "        return [token_idx, label]\n",
        "\n",
        "    def attention_masks(self , encoded_sentences):\n",
        "      # attention masks, 0 for padding, 1 for actual token\n",
        "      attention_masks = []\n",
        "      for sent in encoded_sentences:\n",
        "          att_mask = [int(token_id > 0 ) for token_id in sent]\n",
        "          attention_masks.append(att_mask)\n",
        "\n",
        "      return attention_masks\n",
        "\n",
        "    def spam_collate_func(self,batch):\n",
        "        \"\"\"\n",
        "        Customized function for DataLoader that dynamically pads the batch so that all \n",
        "        data have the same length\n",
        "        \"\"\" \n",
        "        data_list = [] \n",
        "        target_list = []\n",
        "        mask_list = []\n",
        "\n",
        "        max_batch_seq_len = None # the length of longest sequence in batch\n",
        "                                 # if it is less than self.max_sent_length\n",
        "                                 # else max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        \"\"\"\n",
        "          # Pad the sequences in your data \n",
        "          # if their length is less than max_batch_seq_len\n",
        "          # or trim the sequences that are longer than self.max_sent_length\n",
        "          # return padded data_list and label_list\n",
        "        \"\"\"\n",
        "        \n",
        "        # find the max sequence length from the batch\n",
        "        max_batch_s_len = max(len(datum[0]) for datum in batch)\n",
        "        max_batch_t_len = max(len(datum[1]) for datum in batch)\n",
        "\n",
        "        max_batch_seq_len = max(  [  max_batch_s_len , max_batch_t_len ] )\n",
        "\n",
        "        if max_batch_seq_len > self.max_sent_length:\n",
        "          max_batch_seq_len = self.max_sent_length\n",
        "\n",
        "        # pad each of the texts in batch\n",
        "        for datum in batch:\n",
        "          padded_s_vec = np.pad(np.array(datum[0]),\n",
        "                              pad_width = ((0, max_batch_seq_len - len(datum[0]))),\n",
        "                              mode = \"constant\", constant_values = 0)\n",
        "          \n",
        "          padded_t_vec = np.pad(np.array(datum[1]),\n",
        "                              pad_width = ((0, max_batch_seq_len - len(datum[1]))),\n",
        "                              mode = \"constant\", constant_values = 0)\n",
        "          \n",
        "          #generate the attention mask\n",
        "          #attention_mask = self.attention_masks( [padded_vec] )\n",
        "\n",
        "          data_list.append(padded_s_vec)\n",
        "          target_list.append( padded_t_vec )\n",
        "          #mask_list.append( attention_mask[0] )\n",
        "\n",
        "\n",
        "        # convert to tensors\n",
        "        data_list = torch.from_numpy(np.array(data_list))\n",
        "        label_list = torch.from_numpy( np.array(target_list)  )\n",
        "        #mask_list = torch.from_numpy(  np.array( mask_list ) )\n",
        "\n",
        "        return [data_list, label_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmk8zmH5Eudo"
      },
      "source": [
        "#make datasets and dataloaders\n",
        "train_dataset = Seq2SeqDataset( train_source_sentences , train_target_sentences , max_sent_length)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "\n",
        "valid_dataset = Seq2SeqDataset( val_source_sentences , val_target_sentences , train_dataset.max_sent_length)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=False)\n",
        "\n",
        "\n",
        "test_dataset = Seq2SeqDataset( test_source_sentences , test_target_sentences , train_dataset.max_sent_length)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=train_dataset.spam_collate_func,\n",
        "                                           shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRD-mFHeEy7c"
      },
      "source": [
        "x , y = next(iter(valid_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lv8w0qIE1zs"
      },
      "source": [
        "#encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44brnB3EE9Ov"
      },
      "source": [
        "#attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKIrDrM8FC3I"
      },
      "source": [
        "#decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYrPW5AaFD6-"
      },
      "source": [
        "#model class\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg , teacher_forcing_ratio = 0.5 , test_stage = False):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        if( test_stage ):\n",
        "          outputs = torch.zeros(max_sent_length, batch_size, trg_vocab_size).to(self.device)\n",
        "        else:\n",
        "          trg_len = trg.shape[0]\n",
        "          outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        if( test_stage ):\n",
        "            input = torch.from_numpy( np.array([ vocab_to_int_src[ \"start\" ]  ]) ).view(batch_size).to(device)\n",
        "            for t in range(1, max_sent_length ):\n",
        "            \n",
        "              #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "              #receive output tensor (predictions) and new hidden state\n",
        "              output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "              \n",
        "              #place predictions in a tensor holding predictions for each token\n",
        "              outputs[t] = output\n",
        "              \n",
        "              #decide if we are going to use teacher forcing or not\n",
        "              teacher_force = random.random() < teacher_forcing_ratio\n",
        "              \n",
        "              #get the highest predicted token from our predictions\n",
        "              top1 = output.argmax(1) \n",
        "              \n",
        "              #if teacher forcing, use actual next token as next input\n",
        "              #if not, use predicted token\n",
        "              input = trg[t] if teacher_force else top1\n",
        "\n",
        "              i_word = int_to_vocab_trg[top1.detach().cpu().numpy()[0]]\n",
        "\n",
        "              if(i_word == \"END\" ):\n",
        "                break\n",
        "\n",
        "            return outputs\n",
        "\n",
        "        else:\n",
        "            #first input to the decoder is the <sos> tokens\n",
        "            input = trg[0,:]\n",
        "\n",
        "            for t in range(1, trg_len):\n",
        "                \n",
        "                #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "                #receive output tensor (predictions) and new hidden state\n",
        "                output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "                \n",
        "                #place predictions in a tensor holding predictions for each token\n",
        "                outputs[t] = output\n",
        "                \n",
        "                #decide if we are going to use teacher forcing or not\n",
        "                teacher_force = random.random() < teacher_forcing_ratio\n",
        "                \n",
        "                #get the highest predicted token from our predictions\n",
        "                top1 = output.argmax(1) \n",
        "                \n",
        "                #if teacher forcing, use actual next token as next input\n",
        "                #if not, use predicted token\n",
        "                input = trg[t] if teacher_force else top1\n",
        "\n",
        "            return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyVjby4RFMbM"
      },
      "source": [
        "#define hyperparams\n",
        "INPUT_DIM = len( vocab_to_int_src )\n",
        "OUTPUT_DIM = len( vocab_to_int_trg )\n",
        "ENC_EMB_DIM = 300\n",
        "DEC_EMB_DIM = 300\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5GbWd1YFUst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a95c6f-ed88-43b6-b2d2-bd9270ff09ae"
      },
      "source": [
        "#initialise weights\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(6350, 300)\n",
              "    (rnn): GRU(300, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(5759, 300)\n",
              "    (rnn): GRU(1324, 512)\n",
              "    (fc_out): Linear(in_features=1836, out_features=5759, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBe64HxLFY-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34fccd85-34c4-4001-85c3-9364397315b2"
      },
      "source": [
        "#display number of trainable params\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 20,848,015 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rahnWlmFdDZ"
      },
      "source": [
        "#define optimizer\n",
        "optimizer = optim.Adam(model.parameters() , lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IW8ErR8Fd8J"
      },
      "source": [
        "#define loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t9KjCBiFiqR"
      },
      "source": [
        "#train function\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, ( src , trg ) in enumerate(iterator):\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # move to device\n",
        "        src , trg = src.to(device) , trg.to(device)\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        # trg = [trg len, batch size]\n",
        "        # output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G76fTJyOFlSW"
      },
      "source": [
        "#evaluate on dev set function\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src , trg) in enumerate(iterator):\n",
        "\n",
        "            #move to device\n",
        "            src , trg = src.to(device) , trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao5bSHebFopt"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3GBxYPRFwFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d4d9d6-d59d-43b3-cc5b-9b4c2d0ad5f6"
      },
      "source": [
        "#training loop\n",
        "\n",
        "N_EPOCHS = 100\n",
        "CLIP = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Training the model...\")\n",
        "tr = {'loss': [], 'PPL': []}\n",
        "val = {'loss': [], 'PPL': []}\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_loader , optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader , criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'drive/MyDrive/tut3-model-1c.pt')\n",
        "\n",
        "    # store logs\n",
        "    tr['loss'].append(train_loss)\n",
        "    tr['PPL'].append(math.exp(train_loss))\n",
        "    val['loss'].append(valid_loss)\n",
        "    val['PPL'].append(math.exp(valid_loss))\n",
        "    \n",
        "    if( (epoch+1)%5 == 0 ):\n",
        "      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Epoch: 05 | Time: 0m 10s\n",
            "\tTrain Loss: 6.051 | Train PPL: 424.621\n",
            "\t Val. Loss: 6.119 |  Val. PPL: 454.457\n",
            "Epoch: 10 | Time: 0m 10s\n",
            "\tTrain Loss: 6.010 | Train PPL: 407.655\n",
            "\t Val. Loss: 6.116 |  Val. PPL: 453.149\n",
            "Epoch: 15 | Time: 0m 10s\n",
            "\tTrain Loss: 5.978 | Train PPL: 394.689\n",
            "\t Val. Loss: 6.149 |  Val. PPL: 468.239\n",
            "Epoch: 20 | Time: 0m 10s\n",
            "\tTrain Loss: 5.963 | Train PPL: 388.848\n",
            "\t Val. Loss: 6.148 |  Val. PPL: 467.948\n",
            "Epoch: 25 | Time: 0m 10s\n",
            "\tTrain Loss: 5.934 | Train PPL: 377.740\n",
            "\t Val. Loss: 6.150 |  Val. PPL: 468.780\n",
            "Epoch: 30 | Time: 0m 10s\n",
            "\tTrain Loss: 5.912 | Train PPL: 369.305\n",
            "\t Val. Loss: 6.169 |  Val. PPL: 477.870\n",
            "Epoch: 35 | Time: 0m 10s\n",
            "\tTrain Loss: 5.893 | Train PPL: 362.613\n",
            "\t Val. Loss: 6.159 |  Val. PPL: 473.030\n",
            "Epoch: 40 | Time: 0m 10s\n",
            "\tTrain Loss: 5.871 | Train PPL: 354.447\n",
            "\t Val. Loss: 6.161 |  Val. PPL: 473.869\n",
            "Epoch: 45 | Time: 0m 10s\n",
            "\tTrain Loss: 5.851 | Train PPL: 347.566\n",
            "\t Val. Loss: 6.131 |  Val. PPL: 459.920\n",
            "Epoch: 50 | Time: 0m 10s\n",
            "\tTrain Loss: 5.811 | Train PPL: 334.023\n",
            "\t Val. Loss: 6.142 |  Val. PPL: 464.833\n",
            "Epoch: 55 | Time: 0m 10s\n",
            "\tTrain Loss: 5.770 | Train PPL: 320.466\n",
            "\t Val. Loss: 6.147 |  Val. PPL: 467.083\n",
            "Epoch: 60 | Time: 0m 10s\n",
            "\tTrain Loss: 5.723 | Train PPL: 305.700\n",
            "\t Val. Loss: 6.166 |  Val. PPL: 476.277\n",
            "Epoch: 65 | Time: 0m 10s\n",
            "\tTrain Loss: 5.690 | Train PPL: 295.792\n",
            "\t Val. Loss: 6.186 |  Val. PPL: 485.934\n",
            "Epoch: 70 | Time: 0m 10s\n",
            "\tTrain Loss: 5.655 | Train PPL: 285.775\n",
            "\t Val. Loss: 6.208 |  Val. PPL: 496.628\n",
            "Epoch: 75 | Time: 0m 10s\n",
            "\tTrain Loss: 5.606 | Train PPL: 272.031\n",
            "\t Val. Loss: 6.223 |  Val. PPL: 504.027\n",
            "Epoch: 80 | Time: 0m 10s\n",
            "\tTrain Loss: 5.572 | Train PPL: 262.830\n",
            "\t Val. Loss: 6.222 |  Val. PPL: 503.694\n",
            "Epoch: 85 | Time: 0m 10s\n",
            "\tTrain Loss: 5.529 | Train PPL: 251.841\n",
            "\t Val. Loss: 6.276 |  Val. PPL: 531.883\n",
            "Epoch: 90 | Time: 0m 10s\n",
            "\tTrain Loss: 5.501 | Train PPL: 244.923\n",
            "\t Val. Loss: 6.288 |  Val. PPL: 538.336\n",
            "Epoch: 95 | Time: 0m 10s\n",
            "\tTrain Loss: 5.457 | Train PPL: 234.323\n",
            "\t Val. Loss: 6.292 |  Val. PPL: 540.078\n",
            "Epoch: 100 | Time: 0m 10s\n",
            "\tTrain Loss: 5.430 | Train PPL: 228.167\n",
            "\t Val. Loss: 6.302 |  Val. PPL: 545.824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRgLoiKYaegT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "49768946-8bbd-469a-9862-ca27b0566343"
      },
      "source": [
        "#plot train and validation PPL logs\n",
        "\n",
        "fig = plt.figure(figsize=(10,7))\n",
        "\n",
        "y1 = tr['PPL']\n",
        "y2 = val['PPL']\n",
        "\n",
        "plt.plot(y1, \"-b\", label=\"Train perplexity\")\n",
        "plt.plot(y2, \"-r\", label=\"Validation perplexity\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.ylim(100, 950)\n",
        "plt.ylabel('Perplexity', fontsize=12)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.title(\"Train and validation perplexity \\n GRU with attention (no pre-training) - Seeker post + non-empathetic response input\", fontsize = 14)\n",
        "\n",
        "fig.savefig(\"1b-ppl.pdf\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnkAAAHOCAYAAADkLh/xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5xU9fX/8dehLgKKYgcVVFQEqQuKFWwYJWIjatSAWJDEGns0isaW6C+WGDsRNIqFKGg0xRKifhERGxpEAUXBgkiTLgvn98e5swzDlplly+zyfj4e9zEzt37unTszZ879fD7X3B0RERERqVvq1XQBRERERKTyKcgTERERqYMU5ImIiIjUQQryREREROogBXkiIiIidZCCPBEREZE6SEGeSB1gZiPM7O81XY7SmNlHZjasircxzMw+Ku11KcvcbWbjKnvbdVVV7KeZzTSzSypznSISFOSJVCMz83KGERVc9QXAqZVY1LrgNuCgylyhmbVJ3qfCqt7WRqQHcE/qRXJ8T6jB8ojUGQ1qugAiG5nt0p73Ax7MGLc8fWYza+juq8pbqbsvqpzi1R3uvgRYUte2VRWyPc+qgrvPrYntimwMlMkTqUbu/m1qABamjwMKgIVmdrKZvWpmy4EhZtbSzEaZ2WwzW25m/zOz09PXm3m51szGmdk9ZnaTmX1vZt+Z2W1mVupnPsvtlLteM9vazMYm6/jCzAaXdUzMbLcke7NXxvizk200NLP6ZjbczD5P1jvNzC4rZ38yL9/WT8q6IBnuAOpnLHOEmb2eTJ9vZv8ys/Zps3yePL6dlHlcKduqZ2a/NbNZZrbSzD40s/5p01MZwePN7CUzW2ZmU8zssHKO1Tgzu8/M7kzbj1szjn8jM/t98j4uM7O3zaxv2vTeybaPNLOJZvYj0De1D2Z2ppl9mRznMWa2ZTllOj0p+woz+9TMLkqVJzkG35rZ1mnzjzKzd82sUfK6+HKtmc1MZns6KePM5FitsYzsqZmdlZwfjcoqn8jGTEGeSP65mbh8tScwhgj+3iUyfx2AO4H7zeyQctZzClAE7AucC1wInFjG/Nlup7z1jgB2BQ4FjgF+AbQpbaPu/inwdrLezO08lWSY6gFfAT8D2gNXAb8BTid7FwNnAUOAXkSAl7nNpsAdQE+gN7AIeD4tkOiZPB5BZGCPK2VbFwCXApcDewHPAs+YWZeM+W4E7gI6E8fgCTNrVs5+nEIcj17JvpxNvAcpDxOXjn8OdARGJvvQOWM9vweuBvYA3krGtSEu+/cn3r92wF9KK4iZnQXcBFxDvC8XJ/v8y2SWm4BpqXWY2S+Sdf/c3X8sYZU9kseziOPbw91nAi8BmX8WBgOPlrIeEQFwdw0aNNTAAJwQH8Hi120ABy7OYtkngIfSXo8A/p72ehzwZsYyL6Uvk2UZM7dT5nqB3ZJ92C9t+k7AamBYGds5H/gCsOT1jsAaYN8ylrkFeDnt9TDgozJefw1clfa6HvApMK6MbTRNyr5/xntUmDFf5ra+Aq7JmGcc8NeM9QxJm94qGbd/GeUZl5TZ0sZdDcxOnu+SHLcdM5YbA9yTPO+dbOf4EvZhdfqywP7JvO1K2c8vgdMy1nMhMCXj/V8I/AH4ARiaMf9M4JK01w6cUMJnZQFQkLxun8zXsTI+ixo01NVBmTyR/DMp/UVymfEqM5tsZvPMbAmRQdqxnPVMznj9NbB1STPmuJ2y1tueCDImpia6+xfJPGV5AtgeOCB5fTLwubuPTyvfOWY2yczmJmW7qISylbZvmxGZoTfTyrWGtRms1Hy7mNnjZjbDzH4A5hDBYFbbSdaxabIv/5cx6Q0iO5su/VimjlGp71Figrt72us3gVbJdrsBBkwxsyWpATiKCADTTWJ9X7n7l2mv3yLez/aZM5rZVsAORLY3fVu3pG8ref9Tmc3X3P3ecvavJGOBH1mbOR0MTHT3Ot+iWWRDqOGFSP5ZmvH6EuIy2AXAh0QF/5soPxjIrEjvlF1FI9vtZLNeJwfu/p2ZvURcinwteXwsNd3MTiQuo14CjCcyQr8Cjs1lO1n4OzCbuAz6FXFZegpQWfW+Mo9L8bF0dzcz2LBqNPWSbfRg/fdpecbrzPOsItsCOId4T8pyIJEl3MHMGrv7ylw25O6rzOwRYLCZPQWcRlwiFpEyKJMnkv/2B55390fd/X1gBnFZNB+3M5X4XknVXcPMdiQyW+X5KzDAzLoT9dj+mlG2t9z9bnd/192ns35mqlQerY+/AfZJK5dllLMlUT/tJnd/2d0/Bpqz7p/hVP2vdRpsZGzrByIrt1/GpP2JgHFD7Z2UPWUf4Otku+8Rmbxt3X16xvBVFutuZWY7pL3uSbyfH2fO6O5ziP3cpYRtTU/NZ2bHEUH7wcBmRJ3Tsqyi5OP7ENCHqO/XnMj+ikgZFOSJ5L9PgUPMbH8z2wO4G2ibj9tx90+AfxKX8HolDQ1GsH4WqSRjgIbAcOBtjwYZ6WXrZmY/MbN2ZvZbcu+X7k7gMjM7wcx2JzKD6d3XLAC+B84ys13N7CDgPiKbl/Jdsi99zWyb5DJwSW4FLrFoKb2bmV1PXIq+Lccyl2R74A4z292iP7lLgduhuBHLY8CIZD93NrNCM7skCbbKsxwYaWZdzKwXsf8vuPu0Uua/ljimFyXl6WhmvzCzKwHMrBXRTdBv3P01IgN3npkdWkYZZhLn4bZmtnlqZHJuvUEc29FJUCsiZVCQJ5L/biDquP2DuJS5lLRLmXm4nUFEVyOvAs8DjxM/3GVy92VEK9TOrJvFA7gfeCpZ19tEw4X/l2O5/h/R8vQhoq5ZPdL2L6mjdyLQCfgI+DPwW2Bl2jxFRCORM4ks1thStnUXEYz8IVnXsURDhw9yLHNJHiMyXW8RAdRwkiAvcTqxn38gMqt/Jy6XfpHFumcSGbLniffvM8powezuDxH1404DPgBeJ1r7fp5kG0cQ2cVUEPo6UWdvZJI5LcnFRMZuVrJsuuHEpfPhWeyLyEYv1ZJNRETynEW/fB+5+7lVsO5hRKvWjpW97spiZpcDZ7h7VVRXEKlz1PBCRETyWtJ34E5Eo6Aba7g4IrWGLteKiEi+u5voqPv/iEv3IpIFXa4VERERqYOUyRMRERGpgxTkVTLLuFF5KfOkbk5eWNZ8tVmyfydU07auNbNS768pIdf3pKrOUzP7lZk9X5nrrGobw2dWap6Z9U7Osy2rYN06hyuZmY0zs7truhxlyesgz8yOMbOXzWy+mS03s0/NbKSZ9UibZ1By4qaGOWb2vJl1yFhXiW9GsvySSiz2baT132VmI8zs75W4/lKV9iGuyjKUse7tiG4YqpSZbU10uXBDVW+rulVBoJzrezIrWeb9SiwDRBcm3c3sgHLnzELyPfGmmS1Mbq011cweqox111VmNtPMLqnpcmzMqjJAKGXdVfV53pgdB1xZ3RvNJWDP2yDPzG4ERhO3VzqG6In+RKLH+FszZl9GnLzbE/dobAq8YGaVdSuirLn7EnefV93bzTfu/m2uty6qoDOJe1h+Vg3byllVn4Nm1iDj7gelyvU9cffVyTJF5c+dvaQMjxP9zW0QMzsEeJoIXvcBuhKdA2d1TGpSTXw/bagkeBhU0+WQ3FXF59nMGlbWumojd5/v7otruhxlcve8G4C9ifsvnl/KdEt7PghYkjH9p8nye6WNGwfcXcK61ls+Y/oTwH1pr29I1r1P2rhZwKnJ82FEP1ap554x9CY6cnXgeOAlIkidAhxWznE5guhsdAEwH/gX0D5teua2xpVWhmT+Vsn+LUiGF4B2aesbRnTkehJxi6vFxF0Jtixr/9LKckLauvYCXiZ61J9PdJK6Wdr0EUSnrRcQ9wxdQHToukk5x+Qj4IKMceOAe4j7rn5P3KXgNqBe2jybAyOT7SxPytahnG3NTPb5r8R9Xb8FLsmYx4l7qj5DdCZ8W9o5+Q6wgugo+EagUTnbSj+uMzPek0HJe7IaaFbeuZH5npDFOZg2T2Hyunfy+hCiI95lxE3uu2VsZzDwZTL9eeI2VJ4xz4FEJ8Nlvr9ZfFfcAbyRxXz7Av9NyvQVcC+wafp3CnBZckyXE38uTy3jWNQjOkv+nOQzU957nHb+/AVYCDxdSllHEJ+Fq4E5ybn2MNAkbZ7Gyb7PSbY3Adg/bXpDokPmr5PjPAu4Je3zsc7nNofjPQ4YlMP8qX0p9XOdxb5kdd6Vsv1s3pNrknIuTo7TiUAL4rtxCTANOLyE8vQjsmIrkm10T5unJTCKuA/ycuB/wOkZxyXzu7NNtvtKGedzGetuQ9o5nMy7B/AcsCjZ1zdJ+93M2GZq+ZOJjrKXA+cm004nvj9WEHenuYh1v2+HJONXEN/J/wIaVOL5Xu5xI26p9yjxe7CC6Oj7wozpDyTTFyfHt7CkY5Hxebg77fXMZD/uJ+6vPRu4tITv4XOJ39tlRCflpX7XZCx3QtrzdX7rSy1jRb9cq3Igbj+0OHUSlDPvINKCNOLDOSrZ8T1KezNKW76E6ecAU9NevwHMBa5IXu+abKt18noYa4O8ZsCTxI/otsnQKO1NnEp8CbUjAo55QLMyynJ8MrQjeuV/CphO8qVF3JTcgb7JtrYoowybEB+6Ecm69iAuo31B8gWc7MsS4i4EnYBeyfT7y9q/Ek7IpsSPzRgi2Dso2fbfMr70FhE9+LcHDid+CK8s43hsAawB9ivhg7cIuJ649+rPiFtTnZw2z9jk+B+YlOk54gu+SRnbm0l8cK9K1juEuJfpcRkfxO+IDOPOxG3B+ibLnU7cb7UP8AlJAFjKtrZK1nVmcly3SntPlgL/BroBHYn+Lss8N0p4T9pQzjlI6UHexGQf9iC+rD9mbUv9Xsl7cnlyjM4iPi+esX+bEAHqIRv4XXEF8aPRuYx59iLO44uT/dyb+DEbnTbPjcl7ckTynv08Oc5HZR4LIoAaRQTb2yfTy32P086fy4jvjXallHcE8f33dPL+9iV+yO/K+I78hrhy0Z743CwBtkumX0yczwcCOxJBwelpn5tZwHUkn9scjvc4cg/yyvxcZ7Ev5Z53pWw72/dkPvFHpB1xV5QVwIvAL5L3aTjxmS7IKM/UZBsdk/fqG9Z+d7YiMspdiO+Bs4nvikOS6ZsB44mAP/XdWT+bfaWc87mMdbdh3c/z9sRnZyxxj+LdgFOBLqUcz9TyM4ETiM9Ja+Iz/k3auJ8Sf4BTAWAh8f17CtHXYWciCEwP8jb0fM/muP2JCMp7JuXoDQxIphnx2/5CMn1X4HfE+bNdOZ+HzCBvHhHE7Qqcl5SrV8b38Dzi92M34vdkTdr7ss77VMr393q/9aWWcUO+YKtqIG6r9EHGuF8mb2pq2DEZPyjZ2SXEl3Iqsh1b1puRNn4QZQd5eyTr2474YVpJ/ID9K5l+JjA9bf5hJEFe2gn891I+LEPSxrVKxu1fWllKKFtT4ody/3JOjpLKMJj4h5qeFa2fnHw/S9uXFaybcbsqY3/XW3cJJ+RZxBd987TpvZN5dk1bzyygfto8DwIvl7H/XZJ1tC3hvX4zY9xLwEPJ83bJcgemTd8sKeOZZWxvJvBSxriHSMskJev9U8Y8rwG/zRh3THLOlvUjVXwMM86vVcA2uZwbJbwn5Z6DmedT2nvWN22Z/Vj3T84o4J8ZZXmAErJFxI/rGdme72Xs5wtJGWYRVTzOIe3PEvAIMLyUc2frZB3LgQMy5rkDeDHjWBxE3Jt3AmlfrNm8x8n583wW+zSCCITS9+FU4runaTL8CPwi47M7A7gheX0X8Epp51dSlkvKK0sJy40j9yCv1M91lvtS7nlXyrazfU9GpU1vlqw3PcBIvfeZn4NTMpZbSNnfH0+QfAelHcu7M+Ypd18p53wuY92Z+3Ej8ae91CsKpSx/ccb4L4HTMsZdCExJnh9Hxvd/FZzv2Ry354C/lFKGg5PzoknG+PeBy8r5PGQGeaMy5pkGXJ322oEHM+Z5GfhrSe9TxnKZ399lZhrdPX/r5JXgMeJEPpV409PLviyZ1p2IjqcljxvM3acS/0p6E/+GZxDZq/2S+gi9iTe6IianPf86edy6tJnNbBcze9zMZpjZD0Tquh7xTz1X3Yl/XYuTyupLiA/i5sS/3pQv3H1RRjlLLWMp2gOTfd26C+OJfy97po2b4u6rc9hWk+RxRQnTJme8Tl9X+2Tbb6YmJvv4YUZ5SvJmCa8zl5mU8bo7cFXqOCfH+nHiPN7WzH6TPs3Myns/Z7v7nPQRG3Bu5HQOZrHMHsS/6XRvlbKe5ax9D9dhZgdkHJNTSprP3Ze6+1HEv+briB+Lm4H/mdk2yWzdgVMzjv//JdN2Id6/AuCfGfMMZd3PAsSl+i2IjMz8tPFlvsdp82WeG6WZ7O7pDcLeJDLwuyRDw7R9IPncpJ+LI4jvxE/N7M9mdpSZ5fx9b2b3ZezTAcB9OZ6vZX2us9mXlFLPu4zy3JdMy/Y9KV5vcsyXEd8FKanPWubnIv37Ywlp3x9mVt/MrjKzyWY2L9n2cWT/XV3WZ6y88zlbXYk/qD/msAykncNmthWwA3B/RnluSSvLS0Qw+bmZPWZmA82secY6N/R8L15P2vPM43YvcKKZfWBmt5nZQWnzdieSOHMz9qMjuR3TzDKkylHquZP2urzfngrJ19uafQocYGYN3X0VFP8ILzKzFiXM7+4+PXk+1cy2IzIKfdLm+YHI1mRqQQQ3Zflvsq7vgP+4+0wz+55ImR5ExVvXrEo9cXdP6s+X9UX8d+Ia/xAinV1E1IOoSAXuesS/lJNKmJb+47UqY5qXU8Zc+QZs6/vkcXMilZ+uouX28mcp19KM1/WIAOTpEuadC9xHXF5N+bqE+cpaP1T83Mj1HFxnGdYer4qcE1sQ+1+SSUSQkjKnlPmiEO4ziD9gDyWNtj4lgrRhSdkeAm4vYdGviMvbEJeZvsyYnnkevUBcxtuPuGSeUt57nFLSe1eZ4i+/+7tm1oa4nHMIcSn+AzM7zN3X5LC+a4j6rCmPAX8j6pymlHe+VtZnsazzLv1c+SFtWjbvSUnl29Bz/BLicuoFRPC3hKgjnO0f5LK2X975XNXSz+FUmc4h/rivx90Xm1k3ourAYcTv5U1m1sPdyzt3ypP1OeLu/zCznYCfEJ+JF8zsaXc/PZlnDvEnJtMPJYwry4b+ZqY+n8WNxzakgUu+BnmjiJZ35wF/rMDytwO/NrPj3D31ZfQJcKSZmSf5zkS3ZFpZxhEf2DlE3YDUuLOIOgnjylj2RyK1vEHMrCWRJfmlu/8nGdeNdd/D1D+yzO2VVIZ3iQq037v7wg0oWjb79zEw2Myap2Xz9iVO/I83YNsziA/gnkRAk62Pk233Ii7pYGabEnVdHi5n2X1KeF3ePrxL1A+dXsr0+awbWKesIotzJ8tzo7pMJf78pOuZOZOZ7UJkz94taSXuvpyoU1gRM4lsTLPk9btEo5oS12dmU4hLQzu5+6vlrPuhZH1jzKy/u7+Uto2y3uNc7WVmTd099YO6D/FZm5G8/pEINGck+1CfOJ8fT60g+ayNBkab2QjiEvOuRACc1feSu39H/Lkl2c5y4LtK3M8Z2exLFuUsqTyV/Z5k2oeovI+ZNSWyPo8k0/YnLs0/mkw3ov5V+ndtRX8byjyfc1j3e0RGsFEFsnkAuPscM/sa2MXdHyljviKiscarZnYtcU71I6pyQCWc71mW93ui8cWjZvYPYJSZnUMc022ANV49PTXsQ9SZTH+d+h1J/QHZLm16+p8YKP23fj15GeS5+wQz+wNwaxJ5jyb+YW9DVGCFqG9U2vI/WPSTdZ2ZjUn+ud5LVIb8k5k9SFziO5IIdI4up0jjkuV3Ym1AN46oWzLD3WeXsexM4CdmtjtR3628rGFpFhCZq7PMbBZRf+pWImOT8h1xCayvmc0EViQZ0JLK8Bjxb3OsmV1DHN8dgP5Ea+JpWZZrvXWnsq9pHiP+UT+SbGtzovXRMxvyBezua8zsZeILdXQOy00zs7HEJYaziS/eG4mAsbwvjX3M7Mpke72JrE6JlxLTXA/83cy+IDJ2RcQPQk93v6yM5WYCh5jZf4GV7r6glPmyOTeqy13AG2Z2KdHQ5kDg2BLmOwD4LIfzrERmNoy4zPIicUmoBfEHsRlRBwfg98CE5DLe/UQl7z2An7r7kCTTcBtwW/Jj/Fqy/D7El/4DaZvE3R9I5htjZsckgV5F3+PSNAD+YmbXExXkbyHq8SxN9vte4PfJFYXPiYrs2xCtyjGzXxPZ7feJPws/Z21rP4hz6wAz+ytxbqWy4tXK3ZeWty8boLLfk0xXm9lcIpN5DfHDm/r++JS4NLg/8dk8j6ge817a8jOBnknGdQkl/9ErSZnncw7rvofIwD2VZL8XEH/QPnb3XPrSu5b4XV1IfA4bEsmTVu5+s5n1Iy55vpaUow/QnHX/HG/Q+Z6NZN3vEi2dGxCXzz9z95XJ78j/Eb+HlxF/VrclGmK97O6v53A8snGcmb1NxBEnEJnFvSH+4JrZBOByM5tBXIG8OWP50n7r11depb2aHIjWgq8SJ98q4gvqCdIqSFNKwwmi7sMq4Odp43oQLW7mEIHOW8AxWZblG9ZtUNGGSMM+lDHfsIz5tiIu6yxO5u9NFhUrSynDwUSLvhXJY1/iAzwobZ4ziYBtNUmz6pLKkIzfhshcfUdkMj4n/l1sWdK+lHS8y1j3OvtCZMleSU7MBZTShUpZx7KUY9I3eW/SK3aPY/1Kx+usnw3rQmVUctznAJdn8x4SrQpfJzJMPxCXI88tZ3s/JeqXriKjC5UKnhvlVtwtax7WVm7esoTPQXq3DIOJyvbLiS5ULgaWZ2znXyQt1DfwO6IPcTnui2TfvwP+Q9IqNm2+QqLBxA/E5aYPgevTphvxQ5zK6s0l6hIdVsZ+npOsKzVPme8xWTZ2YG2XEtck+7MkOVdL63ZkJet3KXEW8YO2OCnLf4F906bvA3yQHDPP4XiPowJdqJT1uc5iX7I670rZfs7vCet/bgqSbfXLKM/RRP2rlcmx7pG2zObEJe3FyXv4ByIgGZc2z25EXaxlyfraZLuvlH8+l7TuktbTgQjMliRlHQ90LOVYlnrMiWTJu8n5tIBoqXpSMm1/4jM5j/hO+Ij1u5PZ0PO93ONGNBr8X3JM5if7nd4FWXPiSt1sImCfRcQbu5TzechseJF5PmXO40TC6Z/J8fgSGJixTHsi6EzVDz2A9X9T1/utL2lItTASqbXM7E3gHk8ujVThdmYSH9bbyptX1jKz24FD3X2v5HVHIuDfzUv797kRSy6tbunu/Wq6LLI+M+tNBC1beQ1lQOuSje18NzMnum7J+urThqhNrWtFSjMEnct5w8wuNbMuZrZrUt/lHNatf7I90R2CAjwRkSqUl3XyRHLh7pNZv9m61JxCor7nZkQVgCtZ22AJd/93KcuJiEgl0uVaERERkTpIl7hERERE6iAFeSIiIiJ1UJ2pk7flllt6mzZtaroYIiIiIuV65513vnf3rapyG3UmyGvTpg2TJmV7W0gRERGRmpN01F2ldLlWREREpA5SkCciIiJSBynIExEREamD6kydPBERkeq0atUqZs+ezYoVK2q6KJLHCgoKaN26NQ0bNqz2bSvIExERqYDZs2fTvHlz2rRpg5nVdHEkD7k78+bNY/bs2bRt27bat6/LtSIiIhWwYsUKWrZsqQBPSmVmtGzZssayvQryREREKkgBnpSnJs8RBXkiIiK10Lx58+jSpQtdunRh2223pVWrVsWvf/zxxzKXnTRpEueff341lbRsM2fOpGPHjhVa9rnnnuOWW24BYMyYMUyZMqUyi1brqU6eiIhILdSyZUvef/99AIYNG0azZs245JJLiqcXFRXRoEHJP/OFhYUUFhZWSznLK8uGOProozn66KOBCPL69evHnnvuWenbqa2UyRMREakjBg0axDnnnMPee+/NZZddxsSJE+nVqxddu3Zl33335ZNPPgFg3Lhx9OvXD4gAcfDgwfTu3Zudd96Zu+66q8R1N2vWjIsuuogOHTpwyCGHMHfuXABmzJjBEUccQffu3TnggAOYOnVqiWUZNmwYp512Gr169aJdu3Y8+OCD621j9erVXHrppfTo0YNOnTpx//33A3D77bczePBgAD788EM6duzIsmXLGDFiBOeeey7jx4/nueee49JLL6VLly7MmDGDbt26Fa932rRp67zeWCiTJyIiUofMnj2b8ePHU79+fX744Qdef/11GjRowMsvv8xvfvMb/va3v623zNSpU/nPf/7D4sWL2X333Rk6dOh6XX4sXbqUwsJCbr/9dq6//nquu+467r77bs4++2zuu+8+2rVrx1tvvcUvf/lLXn311fXKMmzYMCZPnsyECRNYunQpXbt25aijjlpnG8OHD2ezzTbj7bffZuXKley3334cfvjhXHDBBfTu3Ztnn32WG2+8kfvvv59NNtmkeLl9992Xo48+mn79+nHCCScAsNlmm/H+++/TpUsXHn74YU4//fTKPtR5T0GeiIjIBrrwQkiunFaaLl3gjjtyX27AgAHUr18fgEWLFjFw4ECmTZuGmbFq1aoSlznqqKNo3LgxjRs3Zuutt2bOnDm0bt16nXnq1avHiSeeCMCpp57Kcccdx5IlSxg/fjwDBgwonm/lypUllgWgf//+NGnShCZNmtCnTx8mTpxIly5diqf/+9//ZvLkyYwePbq4/NOmTaNt27aMGDGCTp06MWTIEPbbb79yj8OZZ57Jww8/zB//+EeefPJJJk6cWO4ydY2CPBERkTqkadOmxc9/+9vf0qdPH5599llmzpxJ7969S1ymcePGxc/r169PUVFRudsxM9asWUOLFi2K6waWVZbUMmW9dnf+9Kc/0bdv3/XWNW3aNJo1a8bXX39dbtkAjj/+eK677joOPvhgunfvTsuWLbNari5RkCciIrKBKpJxqw6LFi2iVatWAIwYMWKD1rVmzRpGjx7NSSedxOOPP87+++/PpptuStu2bXn66acZMGAA7s7kyZPp3LlziesYO3YsV155JUuXLmXcuHHccsst67QE7tu3L/feey8HH3wwDRs25NNPP6VVq1YUFRVx/vnn89prr3Huuf8QhvQAACAASURBVOcyevTo4suyKc2bN2fx4sXFrwsKCujbty9Dhw5l+PDhG7TvtZUaXoiIiNRRl112GVdeeSVdu3bNKjtXlqZNmzJx4kQ6duzIq6++yjXXXAPAY489xvDhw+ncuTMdOnRg7Nixpa6jU6dO9OnTh3322Yff/va3bL/99utMP/PMM9lzzz3p1q0bHTt2ZMiQIRQVFXHRRRfxq1/9it12243hw4dzxRVX8N13362z7EknncStt95K165dmTFjBgCnnHIK9erV4/DDD9+gfa+tzN1rugyVorCw0CdNmlTTxRARkY3Exx9/TPv27Wu6GNWmWbNmLFmypMLLl9TNS1W77bbbWLRoEb/73e+qbZslKelcMbN33L1K+7HR5VoRERGpc4499lhmzJhR3NJ3Y1RtQZ6ZXQCcBRjwoLvfYWZbAE8CbYCZwM/cfYFFTcw7gSOBZcAgd3+3usoqIiIi69qQLB5EJq86Pfvss9W6vXxULXXyzKwjEeD1BDoD/cxsV+AK4BV3bwe8krwG+AnQLhnOBu6tjnKKiIiI1BXV1fCiPfCWuy9z9yLgv8BxQH9gZDLPSOCY5Hl/4BEPE4AWZrZdNZVVREREpNarriDvI+AAM2tpZpsQl2F3ALZx92+Seb4FtkmetwJmpS0/OxknIiIiIlmoljp57v6xmf0e+DewFHgfWJ0xj5tZTk19zexs4nIuO+64YyWVVkRERKT2q7Z+8tx9uLt3d/cDgQXAp8Cc1GXY5DHV6c1XRKYvpXUyLnOdD7h7obsXbrXVVlW7AyIiInmkT58+/Otf/1pn3B133MHQoUNLXaZ3796kuhs78sgjWbhw4XrzDBs2jNtuu63MbY8ZM4YpU6YUv77mmmt4+eWXcyl+tctmv0pz5plnFu/vTTfdVJnFqlLVFuSZ2dbJ445EfbzHgeeAgcksA4FUD4rPAb+wsA+wKO2yroiIyEbv5JNP5oknnlhn3BNPPMHJJ5+c1fIvvvgiLVq0qNC2M4O866+/nkMPPbRC66pMq1evLn+mCnjooYfYc889AQV5pfmbmU0Bngd+5e4LgVuAw8xsGnBo8hrgReAzYDrwIPDLaiyniIhI3jvhhBN44YUXim8LNnPmTL7++msOOOAAhg4dSmFhIR06dODaa68tcfk2bdrw/fffA3DjjTey2267sf/++/PJJ58Uz/Pggw/So0cPOnfuzPHHH8+yZcsYP348zz33HJdeeildunRhxowZDBo0iNGjRwPwyiuv0LVrV/baay8GDx7MypUri7d37bXX0q1bN/baay+mTp26XplGjBhB//796d27N+3ateO6664rnvbXv/6Vnj170qVLF4YMGVIc0DVr1oyLL76Yzp078+abb9KmTRsuu+wy9tprL3r27Mn06dPX286MGTM44ogj6N69OwcccABTp06lqKiIHj16MG7cOACuvPJKrrrqKmBtBvSKK65g+fLldOnShVNOOYVrrrmGO9LuaXfVVVdx5513ZvcGVgd3rxND9+7dXUREpLpMmTKlpovgRx11lI8ZM8bd3W+++Wa/+OKL3d193rx57u5eVFTkBx10kH/wwQfu7n7QQQf522+/7e7uO+20k8+dO9cnTZrkHTt29KVLl/qiRYt8l1128VtvvdXd3b///vvibV111VV+1113ubv7wIED/emnny6elnq9fPlyb926tX/yySfu7n7aaaf57bffXry91PJ//vOf/Ywzzlhvfx5++GHfdttt/fvvv/dly5Z5hw4d/O233/YpU6Z4v379/Mcff3R396FDh/rIkSPd3R3wJ598sngdO+20k99www3u7j5y5Eg/6qij3N392muvLd6vgw8+2D/99FN3d58wYYL36dPH3d0/+ugj32OPPfyll17yLl26+MqVK9c7bk2bNi3e1ueff+5du3Z1d/fVq1f7zjvvvM4xSynpXAEmeRXHRrrjhYiIyIa68EJ4//3KXWeXLpCWJSpJ6pJt//79eeKJJxg+fDgATz31FA888ABFRUV88803TJkyhU6dOpW4jtdff51jjz2WTTbZBICjjz66eNpHH33E1VdfzcKFC1myZAl9+/YtszyffPIJbdu2ZbfddgNg4MCB/PnPf+bCCy8E4LjjjgOge/fuPPPMMyWu47DDDqNly5bF87/xxhs0aNCAd955hx49egCwfPlytt56awDq16/P8ccfv95xST1edNFF60xbsmQJ48ePZ8CAAcXjUtnGDh06cNppp9GvXz/efPNNGjVqVOb+tmnThpYtW/Lee+8xZ84cunbtWlz2fKAgT0REpJbq378/F110Ee+++y7Lli2je/fufP7559x22228/fbbbL755gwaNIgVK1ZUaP2DBg1izJgxdO7cmREjRhRfyqyoxo0bAxGYFRUVlThP3PRq3dfuzsCBA7n55pvXm7+goID69euXuo7M9a1Zs4YWLVrwfilB+YcffkiLFi347rvvSpye6cwzz2TEiBF8++23DB48OKtlqouCPBERkQ1VTsatqjRr1ow+ffowePDg4uzVDz/8QNOmTdlss82YM2cO//jHP+jdu3ep6zjwwAMZNGgQV155JUVFRTz//PMMGTIEgMWLF7PddtuxatUqHnvsMVq1ii5rmzdvzuLFi9db1+67787MmTOZPn06u+66K48++igHHXRQTvv00ksvMX/+fJo0acKYMWP4y1/+wiabbFIc0G699dbMnz+fxYsXs9NOO5W4jieffJIrrriCJ598kl69eq0zbdNNN6Vt27Y8/fTTDBgwAHdn8uTJdO7cmWeeeYb58+fz2muv0a9fPyZOnLhe45SGDRuyatUqGjZsCMQ9cq+55hpWrVrF448/ntO+VjUFeSIiIrXYySefzLHHHlvc0rZz58507dqVPfbYgx122IH99tuvzOW7devGiSeeSOfOndl6662LL4kC/O53v2Pvvfdmq622Yu+99y4O7E466STOOuss7rrrruIGFxBZtYcffpgBAwYUN2Q455xzctqfnj17cvzxxzN79mxOPfVUCgsLAbjhhhs4/PDDWbNmDQ0bNuTPf/5zqUHeggUL6NSpE40bN2bUqFHrTX/ssccYOnQoN9xwA6tWreKkk06iVatWXHHFFbzyyivssMMOnHvuuVxwwQWMHDlynWXPPvtsOnXqRLdu3Xjsscdo1KgRffr0oUWLFutlFGuaRd2/2q+wsNBTff+IiIhUtY8//pj27dvXdDHqlBEjRjBp0iTuvvvuCq+jTZs2TJo0iS233LISS1a6NWvW0K1bN55++mnatWtX4jwlnStm9o67F1Zl2aqzCxURERGROmPKlCnsuuuuHHLIIaUGeDVJl2tFREQkLwwaNIhBgwZt0DpmzpxZKWXJxp577slnn31WbdvLlTJ5IiIiInWQgjwREZEKqiv12qXq1OQ5oiBPRESkAgoKCpg3b54CPSmVuzNv3jwKCgpqZPuqkyciIlIBrVu3Zvbs2cydO7emiyJ5rKCggNatW9fIthXkiYiIVEDDhg1p27ZtTRdDpFS6XJul8eOhb1/I40Y0IiIiIsUU5GVp3jz4979h/vyaLomIiIhI+RTkZSlVZ7KC93gWERERqVYK8rLUuHE8rlxZs+UQERERyYaCvCwpkyciIiK1iYK8LCmTJyIiIrWJgrwsKZMnIiIitYmCvCwpkyciIiK1iYK8LCmTJyIiIrWJgrwsKZMnIiIitYmCvCwpkyciIiK1iYK8LCmTJyIiIrWJgrws1asHDRsqkyciIiK1g4K8HDRurEyeiIiI1A4K8nJQUKBMnoiIiNQOCvJyoEyeiIiI1BYK8nKgTJ6IiIjUFgrycqBMnoiIiNQWCvJyoEyeiIiI1BYK8nKgTJ6IiIjUFgrycqBMnoiIiNQWCvJyoEyeiIiI1BYK8nKgTJ6IiIjUFgrycqBMnoiIiNQW1RbkmdlFZvY/M/vIzEaZWYGZtTWzt8xsupk9aWaNknkbJ6+nJ9PbVFc5y6JMnoiIiNQW1RLkmVkr4Hyg0N07AvWBk4DfA7e7+67AAuCMZJEzgAXJ+NuT+WqcMnkiIiJSW1Tn5doGQBMzawBsAnwDHAyMTqaPBI5JnvdPXpNMP8TMrBrLWiJl8kRERKS2qJYgz92/Am4DviSCu0XAO8BCdy9KZpsNtEqetwJmJcsWJfO3zFyvmZ1tZpPMbNLcuXOrdidQJk9ERERqj+q6XLs5kZ1rC2wPNAWO2ND1uvsD7l7o7oVbbbXVhq6uXKlMnnuVb0pERERkg1TX5dpDgc/dfa67rwKeAfYDWiSXbwFaA18lz78CdgBIpm8GzKumspaqceN4XLWqZsshIiIiUp7qCvK+BPYxs02SunWHAFOA/wAnJPMMBMYmz59LXpNMf9W95vNnBQXxqHp5IiIiku+qq07eW0QDineBD5PtPgBcDvzazKYTde6GJ4sMB1om438NXFEd5SxPKpOnenkiIiKS7xqUP0vlcPdrgWszRn8G9Cxh3hXAgOooVy6UyRMREZHaQne8yIEyeSIiIlJbKMjLgTJ5IiIiUlsoyMuBMnkiIiJSWyjIy4EyeSIiIlJbKMjLgTJ5IiIiUlsoyMuBMnkiIiJSWyjIy4EyeSIiIlJbKMjLgTJ5IiIiUlsoyMuBMnkiIiJSWyjIy4EyeSIiIlJbKMjLgTJ5IiIiUlsoyMuBMnkiIiJSWyjIy4EyeSIiIlJbKMjLQf360KCBMnkiIiKS/xTk5ahxY2XyREREJP8pyMtRQYEyeSIiIpL/FOTlSJk8ERERqQ0U5OVImTwRERGpDRTk5UiZPBEREakNFOTlSJk8ERERqQ0U5OVImTwRERGpDRTk5UiZPBEREakNFOTlSJk8ERERqQ0U5OVImTwRERGpDRTk5UiZPBEREakNFOTlSJk8ERERqQ0U5OVImTwRERGpDRTk5UiZPBEREakNFOTlSJk8ERERqQ0U5OUolclzr+mSiIiIiJROQV6OGjeOAK+oqKZLIiIiIlI6BXk5KiiIR9XLExERkXymIC9HqSBP9fJEREQknynIy1HjxvGoTJ6IiIjkMwV5OVImT0RERGqDagnyzGx3M3s/bfjBzC40sy3M7CUzm5Y8bp7Mb2Z2l5lNN7PJZtatOsqZDWXyREREpDaoliDP3T9x9y7u3gXoDiwDngWuAF5x93bAK8lrgJ8A7ZLhbODe6ihnNpTJExERkdqgJi7XHgLMcPcvgP7AyGT8SOCY5Hl/4BEPE4AWZrZd9Rd1fcrkiYiISG1QE0HeScCo5Pk27v5N8vxbYJvkeStgVtoys5NxNU6ZPBEREakNqjXIM7NGwNHA05nT3N2BnO4jYWZnm9kkM5s0d+7cSipl2ZTJExERkdqgujN5PwHedfc5yes5qcuwyeN3yfivgB3SlmudjFuHuz/g7oXuXrjVVltVYbHXUiZPREREaoPqDvJOZu2lWoDngIHJ84HA2LTxv0ha2e4DLEq7rFujlMkTERGR2qBBdW3IzJoChwFD0kbfAjxlZmcAXwA/S8a/CBwJTCda4p5eXeUsjzJ5IiIiUhtUW5Dn7kuBlhnj5hGtbTPndeBX1VS0nCiTJyIiIrWB7niRI2XyREREpDZQkJcjZfJERESkNlCQl6NUkKdMnoiIiOQzBXk5atAA6tdXJk9ERETym4K8CigoUCZPRERE8puCvApo3FiZPBEREclvCvIqQJk8ERERyXcK8ipAmTwRERHJdwryKkCZPBEREcl3CvIqQJk8ERERyXcK8ipAmTwRERHJdwryKkCZPBEREcl3CvIqQJk8ERERyXcK8ipAmTwRERHJdwryKkCZPBEREcl3CvIqQJk8ERERyXcK8ipAmTwRERHJdwryKkCZPBEREcl3CvIqQJk8ERERyXcK8ipAmTwRERHJdwryKqCgANasgaKimi6JiIiISMkU5FVA48bxqGyeiIiI5CsFeRVQUBCPqpcnIiIi+UpBXgUokyciIiL5TkFeBSiTJyIiIvlOQV4FKJMnIiIi+U5BXgUokyciIiL5TkFeBSiTJyIiIvlOQV4FKJMnIiIi+U5BXgUokyciIiL5TkFeBSiTJyIiIvlOQV4FKJMnIiIi+U5BXgUokyciIiL5TkFeBSiTJyIiIvlOQV4FpDJ5CvJEREQkXynIq4BUJk+Xa0VERCRfVVuQZ2YtzGy0mU01s4/NrJeZbWFmL5nZtORx82ReM7O7zGy6mU02s27VVc5sKJMnIiIi+a46M3l3Av909z2AzsDHwBXAK+7eDngleQ3wE6BdMpwN3FuN5SxXgwZQr54yeSIiIpK/qiXIM7PNgAOB4QDu/qO7LwT6AyOT2UYCxyTP+wOPeJgAtDCz7aqjrNkqKFAmT0RERPJXdWXy2gJzgYfN7D0ze8jMmgLbuPs3yTzfAtskz1sBs9KWn52MyxuNGyuTJyIiIvmruoK8BkA34F537wosZe2lWQDc3QHPZaVmdraZTTKzSXPnzq20wmZDmTwRERHJZ9UV5M0GZrv7W8nr0UTQNyd1GTZ5/C6Z/hWwQ9ryrZNx63D3B9y90N0Lt9pqqyorfEmUyRMREZF8Vi1Bnrt/C8wys92TUYcAU4DngIHJuIHA2OT5c8Avkla2+wCL0i7r5gVl8kRERCSfNajGbZ0HPGZmjYDPgNOJIPMpMzsD+AL4WTLvi8CRwHRgWTJvXlEmT0RERPJZtQV57v4+UFjCpENKmNeBX1V5oTaAMnkiIiKSz3THiwpSJk9ERETyWdZBnpm1rMqC1DbK5ImIiEg+yyWT96WZjTWzE5J6dRs1ZfJEREQkn+US5LUhbj12OfCtmT1gZvtXSalqAWXyREREJJ9lHeS5+1x3v8vdewC9iD7tHjWzz8zsejPbqcpKmYeUyRMREZF8VtGGF9smw6bADOKWY++Z2RVlLlWHKJMnIiIi+SzrLlTMrANwKvBz4rZkI4HO7j47mf47YDJwSxWUM+8okyciIiL5LJd+8l4DRgED3H1i5kR3n2lmd1RayfKcMnkiIiKSz3IJ8o5199cyR5pZz1TQ5+7XVFrJ8pwyeSIiIpLPcqmT9/dSxv+zMgpS2xQUwOrVUFRU0yURERERWV+5mTwzqwdYPDVLnqfsAmyUYU7jxvG4ciU0qM47AIuIiIhkIZvwpAjwtOfp1gA3VmqJaomCgnhcsQKaNq3ZsoiIiIhkyibIa0tk7/4LHJg23oG57r68KgqW79IzeSIiIiL5ptwgz92/SJ5uVJ0dlyc9kyciIiJ5bOlSGDsWjj0WmjSp6dJUmzKDPDN7wN3PTp4/Utp87v6Lyi5YvlMmT0REpBZYvBiOOgpefx06d4ann4Z27Wq6VNWivNa1n6c9n1HGsNFRJk9ERCTP/fADHHEEjB8Pv/kNzJoF3bvD3/5W0yWrFmVm8tz95rTn11V9cWoPZfJERETy2MKF0LcvvPsuPPkkHH88DBkCP/sZnHACXHgh/P730KhRTZe0ymTdT56ZXZ10oZI+bhMzu7/yi5X/lMkTERHJU/Pnw6GHwnvvwejREeAB7LgjvPYanH8+3HEHHHggjBgBH35YJzu+zaUz5COA/zOznQHMbF/iXrWbVkXB8p0yeSIiInlm+fII4g49NAK3Z56B/v3XnadRI7jzTnjqKZg2DU4/HTp1gubNYe+9YejQWEcdkEs3vgcCVwJvm9mLQF/gAncfVSUly3PK5ImIiNSwb76BN96IOnfjx8el2aKiaEE7dmzUxyvNgAFw3HER6L377tph1Cjo2DGyfLVc1kGeu68xs78BpwEnAM8DY6uqYPlOmTwREZFq9u238N//wrhxMUydGuMLCqBnT7jkEth33xhatix/ffXrwx57xPDzn8c4d1i1qqr2oFplHeSZ2bnAdcANwCPAPcAHZnaau0+oovLlLWXyREREgDVr4OOPIwD7/vu1w/z50LAhbLLJ2qFp0wjA2rfPbRsLFsDgwTBmTLxu3hwOOCDGHXQQdOlSeQ0ozOpMY4xcLteeARzo7v9LXp9oZqcRGb2tKr1keU6ZPBER2ahNmwaPPhrDzJnrT990U1i9GpYti+xYuk6d4MQTY9hll7K3M2lSXFr96iu4+mr46U+hWzfdOD4LuRyhnu6+Tv7S3R81s3GVW6TaQZk8ERHZ6MydG33MPfIIvPkm1KsXjRyuuQZ23RW23DKGzTdfG4S5x4/lsmWRkXvxxejS5KqrYigsjC5Njj0Wdttt7bbc4YEHoiXsNttEZ8Z7710z+11L5RLkFZnZWcDJwJbu3snMDgS2BWZVSenymDJ5IiKS9z77DLbaKi5vVtSsWfDsszG89lpcnu3QAf7wh6jH1qpV2cubRUOIJk2intz558fw5ZfRwvXJJ+GKK2Jo3x6OOQb69YP77ossYd++8Ne/RvAoOTHPTKGWNqPZ74DDgDuA+9y9RdKdytPu3r0Ky5iVwsJCnzRpUrVtb9WquGT/u99F9lhERCSvvPoqHHlk1Fd7442yL2/+73/R3ciyZTEsXx7DJ5/A22/HPB06RGvU446L24Ot23Xuhvnyy2gNO2ZMNKxYvTrWf+218SNbv37lbStPmNk77l5YldvIJZM3COjq7t+b2b3JuM+BnSu9VLVAgwZx/imTJyIieWfCBDj66Lhs+tZbkXX7zW9Knnf2bOjdOxpLNGwYGbdNNonHbbeFm2+OS6m771515d1xRzjvvBjmz4d//hPatoVevapumxuBXIK8+sCS5Hkq/dcsbdxGxSzq5alOnoiI5JXJk+EnP4kA7fXX4YILYNgwOOqoyMClW7UqGj8sXw5TpuTe6rUqbLHF2u5MZIPkcseLF4E/mlljgOQWZ78jWtdulBo3ViZPRETyyKefwmGHQbNm8PLLsN12cM89ETiddtr6P1qXXRadCA8fnh8BnlSqXIK8XwPbAYuAzYgM3k7A5VVQrlpBmTwREckbX3wRLV3dI8Br0ybGb7klPPRQ3OZr2LC1848eHfdvPe+8yOZJnZPLHS9+AI41s22AHYFZ7v5tlZWsFlAmT0RE8sJHH8U9WhcvjjtBZNaf69cvOg7+wx+in7ktt4zX++wDt91WI0WWqldmkGdmJWX65iZD8XR3X1P5Rct/yuSJiEilefJJuPzyaCyxww7RGGHHHWHnnaM+XZMmJS83ciQMHRqdD//zn+vXu0u5/XZ45RUYODB+wBo1ii5M6sjdHWR95V2uLQJWlTGkptd9r7wCe+0VrZASyuSJiEilmDgxgq9NN41+52bOhMcei6BvwADYaafos2v+/LXLLF8OZ54JgwZFJ8Hvv192Z8GbbgojRsD06dFlyuOPRzApdVZ5l2vbVkspaoNmzSIdPnEitG4NKJMnIiKV4Ouvo4uS7baLvu3SO/1dvDh+d/74x7irxO9/D2edFR0GX3ABfPBBdI1y3XXZ3eard2+4//7IUhx+eJXtkuSHMs8Id/8ic1zSqnZL4HvPtifluqBz5+g/aOLE6AgSZfJERDYKRUXwn/9E69PkT37Wli2Lxg3z50dL1q23Xnf6ihUR4C1aFLcJy7yrQ/PmcMghMXz4Idx6K9x9d6xziy3ghReiw+NcnH12bvNLrZV161oza2FmjwIrgDnAcjN71My2qLLS5ZOCggj0Uj1/o0yeiEidVlQUt9Xac8/IerVrF/da/eGH8pd1hyeegD32iGXuuCPuy/qnP8V6U/OcfXYkDx59NKoElWWvveKesTNmRAOK997LPcCTjUouXag8DDQBuhCdIHcFGgN/yWZhM5tpZh+a2ftmNikZt4WZvWRm05LHzZPxZmZ3mdl0M5tsZt1y2quq0rNnBHlrop2JMnkiInVQUVHcK7VDB/jFL+LuD48+CscfDzfdFMHeffetDdYyvfMOHHAAnHxyZOZeey2q+/TsGfds7d49Oin+4x9jvdddF9m8bO24I1x6aTyKlCGXe9cuArZ19+Vp4zYBvnb3FlksPxModPfv08b9AZjv7reY2RXA5u5+uZkdCZwHHAnsDdzp7mXUJq2me9eOHBkVXJNewQcMiLqrU6ZU7WZFRGQDrFkDc+fG/VGnToWPP147fP551GUrKFg7LF8O33wDnTpFv3L9+0O9JCfy9ttw8cURpLVvH/3SLVkCS5fG48KFay+73nQTnH762vuuusf9YS+6CGbNilsnHXdctHCtl0vOReqCfLt37VSgDfBx2rgdgU82YPv9gd7J85HAOKJz5f7AI0mdvwnJpeLt3P2bDdjWhuvRIx4nToT27ZXJExGpaR9/HJmzhQtjWLQoHufNiwYNX38dAVt61q1Bg8jGdegQ93d1j7o3y5fHsHo1/Oxn0bghM/jq0QP++18YOxauvjoycc2aQdOmax8vuwyuvBI222zdZc0iG3jEEXDLLZEhGDFCAZ5UmVyCvFeAfyf18mYBOwCnAo+a2eDUTO5e2uVbT5Z34H53fwDYJi1w+xbYJnneKtlGyuxkXM0GebvvHpVgk6buqpMnIlIJ5s2LgCib1qEAX30Fo0ZFFyPvv7/utE02gRYtolHC9ttHtm377WNo3TrqyO28czSkqyizCACPOaZiyzdtGt2hiFSxXIK8XsD05LFXMm4GsG8yQARypQV5+7v7V2a2NfCSmU1Nn+jungSAWTOzs4GzAXasjroJ9etDYWEEeahOnohIhaxZE5c9n38e/v736AakTZvoEuSMM+LPdKa5cyN7NmpUtHR1j6zaHXdA377QsmUEiurYV6RYVkFe0m3KGcCX7l5KTdOyuftXyeN3ZvYs0BOYk7oMa2bbAd8ls39FZApTWifjMtf5APAARJ28ipQrZz17RmXZFSsoKChQJk9EJFvz50efbs8+C999F5cp99sv6r29/HLUVbv22mhxet55kTF79tmox/b66xEc7rJL9Bf3859Ha1URKVVWQV6SZfsQKOHvVfnMrClQz90XJ88PB64HngMGArckj2OTRZ4DzjWzJ4iGF4tqvD5eSs+esGoVdEhlwQAAIABJREFUfPABjRvvrUyeiEg2UvdWnT07Ghv89KdRN22LpBeua6+NqyS33x7DH/9Y3JMBHTpENyTHHRddWZnV3H6I1CK5XK59D9iNaICRq22AZyMhSAPgcXf/p5m9DTxlZmcAXwA/S+Z/kWhZOx1YBpxegW1WjZ4943HiRAoK9qaoKOrophpPiYhIhmefhdNOi8uw48ZBr14lz9ezZ1yO/f3v4aGH4l6txx0X9aFFJGe5BHnjgH+a2QiiUUTx5dEyGlukpn8GrHfHZHefBxxSwngHfpVD2apPq1Zx65mJE2ncMUatXBl1fUVEJM2aNdHAYNiwCOCeeSa+Q8uz445w/fVVXjyRui6XIG8/4HPgoIzxZTW2qHvMijtFLkh6t1mxQkGeiGykUh0HP/NMtEZr3nzt8N578OKLMHBgdB5cUFDTpRXZqGQd5Ll7n6osSK3SsyeMHUvz1QuBFqqXJ7WLe3Q90bVr1HUqy8qVMb9+nDcu8+fDp5/G4B43td9pp3XnWb0aHn88Mm7Tp0e3JI0bw+LFa4eGDaP16/nnqx6dSA3IJZOHmbUk6spt6+63mtn2RIOK2VVSunyV1Mtr9c0k4FC1sM1Hc+bEj9O229ZsOZYvh6efhr/8JQKlE0+M2xe1KPcmMVVj1apouThiRNR3evBBOOWUkud94w049dT4MR89GvYu86YzUlu5w6RJEbBNmBCB3fz568+3885w8MExrF4NN9wAn3wSDSGefTYaVaQHcu4xX7Z934lIpcu6m20zO4i4u8UpwDXJ6HbAvVVQrvxWGNdpt/0y+stTJi9PuMMrr0SP8q1awQ47wJAhcfug6vbJJ/DrX0c5Bg6MHvc//RQGD4ZttolOVJ98EpYtq74yLVkSLRpHjIDLL48+xk49NfomW7Vq7XxFRdFFxUEHRYuihg3jPpz33BPHWOqGzz6L+nJ77BF/XO+5J/6IDBgA/+//RR92U6fC5Mlw552w117xh+XnP49GFI0awd/+Bu++G+dzZqbOTAGeSA3L5d617wGXuPsrZrbA3Tc3swLgC3ffprzlq1q13Ls23e6783WL9rSaOIb33oMuXapv05JhwYIIXO67LwKpli2jQ9UlSyJTZRbZq9/8JhrNVIbvv4/sx3vvRSetixbBDz/E49y50Qt/gwbRMnDo0AiYIDqAHTUqArxvvokgcMSIuP9lVfr2WzjqqOh09r774MwzI7C7/PLormL//eP+mcuWReA3YUIEp3/6UwR9p50GL7wQWb/7748e+6V6LF0K48dHBi3XZvxr1kSwNmVKZLdTw9dfxx8RiEuxp5wSf44237zs9a1eHef8woVRHt2OS6TCquPetbkEeQvcffPk+Xx338LM6gFz3b1lVRYyG9Ue5J12GiteeJkmC75mwgTTlaya8uST8MtfxuWlffeNgOqEE9bWIfvyy7is9PDDEXSdfHLcdHy33aJbhp12yi7bMHdu3KNywoQI1GbOXDutWTPYdNPobT/1eNBBkbUr7XLx6tXRa/9550W25IIL4Oab4xJqOvfoO+zNN6OcDRtGBqVRo/iBXbgw9n3BgnhcujRu3bTrrnFvznbt4pLxkUfGj/tTT0Wwl27UqAj6mjeP5evXj0DuxBPXzrNmTZTvt7+FPfeMrE+TJtHqKDUsXBg3e//ss7XDjz/Gsc/cpmTnq68i+5r6J/mnP0VAno1334Vzz41zB+L93WabtUPPnvF5qI67BYnIeqojyMPdsxqA/wP6Js/nJ4+HA+OyXUdVDt27d/dqdddd7uCtmOXjxlXvpuucjz92v/FG9//+133NmuyWmTvXfcAAd3Dv0cP9nXfKnn/GDPdBg9xbtIhlUkOjRu5dusT2P/98/eVmznQ/7zz3Jk1i/jZtYrt/+IP7f/7jvmhRrnu7rqVLY/3g3r792v2YNcv9ppvc99hj3fKWNmyyiXurVu7t2rkXFKw/fcst3d96q/RyTJ4c2+rTx/2LL0qf79//dm/ZsvRymLm3bu1+4IFxvLt0ca9f3/3/t3fnUVKWZ/rHvzfdzQ6yCbJKNyA7gkEENUbBfdcxLtnQaEgyzBkzJib5OXNiYjKOOjOJk3GiMRpF46gEN4i74LiDgkwQhGbfWjZplgZElr5/f9xVUw0CdmF3Vffb1+ec57xVb71d9ZRl0Vc/6333fbH/Tg3RrFnuXbq4t2zpfsst7t27x3/jq66K/z8OZuNG9+99Lz6Ljh3dH3jAfceOnFVbRKoHmOm1nI2yack7AXg2VS4HHgIuAC5y9/dqOnxmK+cteTNmwMiRXMoTfO/FSznzzNy9dCK4w1tvwR13RHdSWu/e0QI2dmxsKH4gzzwT3a+bNsX6Wz/+cfXH/rhHV+vChdFdtXBhTDB46614/KSTYszRsGFw990xGL1Ro+jCvPHG2Oy8Nrz0ElxzTWz1NGJEtL64x1i4b30rWnMKCqJlbNeu6GrduzcmcLRtG7Ma0yoroztu0aKY9fjRR1H/Xr0+/79NdWZArl0b3YdNmkSLafrYqlW0jFadibttW4zxeuGF+Kx+9rNkzrKsrIz/n95+O1pee/aM1tEjjzy853v66ehC7dAhvh9DhkRX+u23RykogB/+MLOtl1mUNWuixXXz5mjF+8UvomVZROqcOtFda2bNgX8CBgFriD1kOxELIv/J68jM2pyHvJ07qWzVmjv23EDvP9/GZZfl7qXrlPJyePHFWAtr0aL4Rd+6debYunUEkapl48ZYVmH69Bg/93d/FwHntdfg/vvh9dcjWJ11Vkye2LMnQs2ePdFt+sor0XU1YUL88qsJy5dHt+Ujj8C8eXGuRYsIk//wD1GP2lZeHt22s2dHl/M3v/n5waw+qDqj97rrIjzX9wH57vH/yZQpsafq9OnxRwdEqNqyJcLv174WXfLDhu3789u3RygsL//sd+bee+MPl+OPjz9o9u/yX7YMfvSjWJfuQE45Be66KyZKiEidVVdC3gPAcOB5YvmU/3H3v6vNSh2OnIc8oPJLx/PmX1tx/9enMWFCTl86P7ZvjzFCq1dHS+azz0aLU2VlhLWhQ6O1oaIiJiFUVMQvu/T+k1WVlMTs02uu+exK0osWxTiuxx+P10yPRSssjLFol10W+1g2blw773POHJg1Cy68MN6XfHHu0Yr3q1/F+Lxf/CKCTy4H7n/ySUw8Wb4cVqzIHDduhI4do+W4a9fMrjbt2kUrafqPk4KCCHSTJ0f4WrYsnnfAgBgPmi7HHBPjLO+6K/4Q2b49xtENGxbBbsGCGCt6KJdfnlnm5mDKyuI9pTvLIb4jxcXJbC0VSZi6EvLWAMe5+xoz6w687u7FtVmpw5GPkMf48Xzyh4fpWLSZNesa0bJlbl++1pWXR5h64434hbJ5876PH3dcDOg/77xodTjQzD/36LLbvDlTKivjl542/G14fv97GD8+upo7dozW2nPOgTPOiK7JmrZ8ebQyP/ccTJsWoSitXbvoXu7QIVqIy8rieDAFBVHvJk1gzJj4I+CCCw4+rADi//cHHojAt25dLFdStXTsGN+P9B9FFRXRxfv1r2vmqkjC1ZWQt9XdW1e5X+7u7WqzUocjLyFvwgS4+mqu4w+ce9tXuPRHJckJLk89FbNWN2yIINejR8za7No1jv361dxyJNKwrF8fYxCffz66+jdujPMHWmetR4/odhwyJMrgwTGbOb1cTfqY/gNi06YomzfDBx/A/PnxXL16xR8jo0fH7aOPju7R/X36aYxrW7Mm8zzp47Zt8cfMmWdyWH/RVXfMo4g0CHUl5O0AzgPS/zo9DVxU5T7uPq22KlhdeQl5y5fj/ftj6S0vmjaNgfmDBsVA9zPOyN0/6jt3RlfYn/8cXaA/+MHhbUW1fn2MIZo4Mbpf//jHz44nEqkpe/dG1/irr0a35v6PLV0a3eelpXH/8xQVRRdr27YR5M45J/5I6dNHAUtE6pS6EvKWA4e6yN29pCYrdTjyEvIAtm/nvhs+5O1753Lnd+bReuXc+KX18ccwciTcfHN0SdXmL5hp02Jnh8WLo7VjzpyY3fev/xoLnB7qtSsqopuqrCwWTL3llmgdufnmmE1aVFR79Raprp07YyzbBx9Ea1vVNQnTx7ZtYwybwpyI1AN1IuTVF3kLecSwn+LiaEj7x38kfgk9+CDcemsMsB4xIkJTv36xxdaqVXF+9eqYwTl0aOz/2LfvvqGqoiImISxcGF1FxcXR1dS9e3QLb9wYs+wefDDO33NP7JwwdWrMCP3gg1iC4/bb43kXLMiU0tKow9at+76ZE06I1rsBA3L3H1BERKSBUcjLQj5DHsTOQOmdgv6vIWHXrhi3d+ut++6QkNa2bXRR7doV9xs3hoEDI/gtWhQDtQ+kqCgC38aNMR7pxhtjJ4KqM/H27o3lSP7pn/YdTF5QEGvR9e0brX3p2YTpsXYlJRrwLSIiUssU8rKQ75D3wAOxhu/bb8OoUfs9uHt3bOT9ySfRCte9ewSqFi3isdLSWNohXXbuzGxJ1adPLMnQqlWMT1q8GJYsiVJZGQvMHmo9rC1bYpxehw7RklhSUntLj4iIiEi1KORlId8hb+vWWLN07NhY61VERETkYHIR8tQvV0Nat4ZLL4XHHouGOBEREZF8UsirQWPHxnJaVbdiFREREckHhbwaNHp0zF9oEFuciYiISJ2mkFeDCgpiT/kXXoC1a/NdGxEREWnIFPJq2NixsXpJSQmccgr85Cfw9NMHXw1FREREpDYo5NWwfv1iLeJx42L5u9/8Bi65JGbefvvbsWKKiIiISG0rzHcFkmj06CgQM23ffx8mTYrA9/HH8Pjj+65bLCIiIlLTFPJqWdOmcOKJUfr0gfHjYyvbKVNiu00RERGR2qDu2hz6/vfh0Udh+nT4ylc0Tk9ERERqj0Jejl1xRbTiLVoEJ58Mr70GZWUxWUNERESkpqi7Ng/OOgteeQXOOw9OPTXOFRRA586xrW2nTtGV27p1HI84Avr2jesbKZaLiIhINSjk5cmoUTB/PsycCatXZ8qqVbBkCWzZEmXrVkhvL3zssfCrX0XYM8tv/UVERKRuU8jLo06dIrAdSmUlbNsWXbw33wwXXBAB8dZbM62AIiIiIvtT518d16hRdNt+/evR8vf738PKlXDaaTBmTCy0vGdPvmspIiIidY1CXj1SVBSLLC9aBP/+71BaGgst9+wJP/95dPeKiIiIgEJevdSsGdxwAyxfHi15gwfDLbfA0UfDhRfC5Mlq3RMREWnoFPLqscJCuOgieP75mKzxk5/Au+/Gue7d4ac/jVY/ERERaXjM01M367nhw4f7zJkz812NvNu9G557Du6/P45798LIkdCxY9xOl0aNYOxYuPJKzdQVERHJNTOb5e7Da/M1ctqSZ2YFZjbbzP6Sul9sZjPMbLGZPW5mjVPnm6TuL0493jOX9azPioqiJW/y5FiO5bbbYgmWlSthzRrYuBEqKqLl72tfi+5djeUTERFJnlx3114PzK9y/3bgN+7eG9gEXJs6fy2wKXX+N6nrJEudO0cX7vTpMHs2zJoV3bnvvAMLFsCvfw1Tp8KAATFrt7Iy3zUWERGRmpKz7loz6wZMAP4ZuAG4ANgAHOXue8xsFPBzdz/LzF5M3X7HzAqBtcCRfojKqrv28CxdCt/5DkybFvvpnnMOlJfDpk2Z48CB8IMfQElJvmsrIiKSDEnrrr0T+DGQbi9qD2x29/Q80NVA19TtrsAqgNTjW1LX78PMxpnZTDObuWHDhtqse2KVlMQWa/fdB3/9a0zWuPPOWHx5/nzYsQPuuQf69IGrroL33893jUVERKQ6crLjhZmdD6x391lmdmpNPa+73wvcC9GSV1PP29CYwbXXwje+EZMymjXbdzJGWRn89rcR9h57DE4/Pa7t3h26dIGuXaFVqwM/9549ma3aliyJlsPy8ljM+bzzYl9eERERqXm52tbsJOBCMzsXaAq0Bv4DaGNmhanWum5AWer6MqA7sDrVXXsEsDFHdW2wmjQ58PmuXeH22+Gmm2Ls3p13wtVX73tNy5bQpk2ExD17MmXHjjiX1rgxNG8eLYdFRTB6NFx8cWzX1qZNjAtMF4hzmv0rIiKSvZwvoZJqyfuRu59vZn8GnnD3x8zsHmCOu//OzMYDg939e2Z2JXCpu19+qOfVmLzc2b0bli2LFr6PPsqUzZsjuBUWZkrz5lBcDL16Rema6pCfPj0Wcn7qKVi8+OCvNXBgbOl21VWxs4eIiEgS5GJMXr5DXgnwGNAOmA18w90/NbOmwMPAMKAcuNLdlx7qeRXy6id3mDcvZvnu2hXr96XLzp0xNvCtt+Lak06KwHfiidCjh1r5RESk/kpkyKstCnnJtXw5/Pd/wyOPwIcfZs63aBFhr3v3WAZm1Kgo3bvnraoiIiLVopCXBYW85HOPGb/z58fizitXxqSOFStg7txo+YPoEh41CoYOjS7enj1jX9/OnaGgIJ/vQEREJOQi5OVq4oXIF2YWLXYDBnz2sd27YwmYd97JlEmT9r2mqCh+9sorY7ePHj1yU28REZF8UEueJNb27dHat2JFdPkuXw5vvAFvvx2Pn3JKjPG77DJo1y6fNRURkYZG3bVZUMiT6lq6NMb4/elPUFoa57p0gb59M6VnT/j00wiK6bJ7NwweHF3BnTod/PkrK2PiiIiIyMEo5GVBIU+y5R47eLz4YoS9dNm8+fN/tlevmOU7YgRUVERwTC/4vHp1bBF3550wZEjtvw8REal/FPKyoJAnNcEdNmyICR1Nm8Yizy1aRDGLUPj225mybl38XMeOmbUAO3aECRNi39/vfhd++Uto/5lN+UREpCFTyMuCQp7kmnssAn3EEREGqyovh5tvhrvvhtat4ZZbYNy42PHjUPbsiR1CDrb7iIiIJINCXhYU8qQumjsXrr8epk2LlsCjjoolXrp1i2IWO4esXh1l7doIjz17Qv/+MRu4f/8YJ1hcHD+v8X4iIvWfllARqecGDYJXXoHnn4cZMyLIlZXBokXw6qsR6Lp1i+A3cGAm+C1YEOsBTp0aE0DSmjSJNf+Ki6F375ghPHo0dOiQv/coIiJ1k1ryROqwvXtj6ZfS0jguW5Y5lpbCtm1x3dChMGZMBL6BA2PXD7X4iYjUXequzYJCnjQ0e/bAzJnRUjh1akwE2bUrHmvSJCaB9O4dpVOnmPzRrl2m9O4NzZrl9z2IiDRUCnlZUMiThm7HDnj3XVi4MLqDFy+O45IlmS3fqioqguOOi6Vg0qVLl9zXW0SkIVLIy4JCnsiBucdizuXlmbJhA8yeHa1/772XCYHt2sV4v+LimPxRXByzhz/5JK5Jl7Zt4cILo1tYRESyp5CXBYU8kcOzaxf87/9G4CstjfF+y5bFdnBVJ30cyPHHw6WXRjnmmNzUV0QkCRTysqCQJ1KzKitjSZdt22LsXrNmsUB006axw8dTT8ETT0RLIMQyLyeeCCecEGXQICjU/H0RkQNSyMuCQp5IfqxcCU8/HdvDzZgBGzfG+ebNI+g1axbLwjRqFKWoKLqBq+4V3KNHdCtv2hQ/v3Fj3O7fH0pK8vv+RERqg0JeFhTyRPLPPVr5ZsyIMm9ezAKurMyUnTtjMsjWrZmfKyqC3bsP/JwDBsD558MFF8CoUVBQkJv3IiJSmxTysqCQJ1J/uMP69TEGsLQ0Ql/TprHMS7q0bh2zhadMgddei7DYvj2cdVaUM86Azp3z/U5ERA6PQl4WFPJEkmvLlugOnjIFXnopAiLA4MFw5plw0UVw8snRLSwiUh8o5GVBIU+kYaishDlzIvS99BK8+WbMEO7XD667DsaO1TZvIlL3KeRlQSFPpGHatg0mTYJ774V33oHGjeGSS+Bv/ib2BD7qqNjxo0WLfNdURCRDIS8LCnkiMncu/OEP8PDDMTu3qhYtoE8fGDcOvvUthT4RyS+FvCwo5IlI2s6d8OGHsG5dpqxdC6+/Du+/Hzt2fPe7MH48dOuW79qKSEOUi5CnpUpFJHGaNo19effnHmP47rwT7rgD/u3f4OyzoVWrmL27e3ccCwrgvPPgiitilq+ISH2kljwRaZCWLYP//M+YsQuxVl9hYZQtW2K9v2bN4KtfhWuugVNOicWcRURqgrprs6CQJyI1xT3W6PvjH+Gxx2Lh5uJiGDky9ujt0ydzbNMm37UVkfpIIS8LCnkiUht27IAnn4ywN28erFgRITBt2LDo1r3iCujZM2/VFJF6RiEvCwp5IpILO3dGV+7ChRH6Jk+OVj+AESMi7J16KvTqBUcckdeqikgdppCXBYU8EcmXZctg4kR4/HGYPTtzvn17KCmJwNe3LwwZAsceG12/Gt8n0rAp5GVBIU9E6oLFi2NHjiVLosVvyZIoy5fHbh0ALVvGlmxf/jL87d/C0UfntcoikgcKeVlQyBORumzHjujenTMnyl//Cm+9FeP7Lr8cfvhD+NKX8l1LEckVrZMnIpIQzZvD8cdHSVu1Cn77W/j97+HRR2Ms39VXx9p8BQWZ0rw59O4NnTuDWb7egYjUN2rJExHJs61b4b77YpHmVasOfl2rVjG2r29fGDAAzjwzWv8U/ETqH3XXZkEhT0Tqu927YdGiOO7dG7tv7N0L27bF+QULoLQ0jitXxs907w4XXwyXXBJj/ArVPyNSLyjkZUEhT0Qako8/hr/8BZ56Cl56KZZ2ad8eLrsMvvY1OPlkzeAVqcsSE/LMrCnwOtCEGAc4yd1vNrNi4DGgPTAL+Ka77zKzJsBDwJeAjcAV7r78UK+hkCciDdW2bfDii/DEE/DMMzHJo0cPuOqqCHxHHgkbN2ZKeTm0aweDBsXyLgUF+X4HIg1PkkKeAS3cfZuZFQFvAtcDNwBPuvtjZnYP8Fd3v9vM/hYY4u7fM7MrgUvc/YpDvYZCnohIBL5nnoFHHokWvr17D31906Yxvm/QIDjxRLj00giFIlK7EhPy9nlBs+ZEyPs+8CxwlLvvMbNRwM/d/SwzezF1+x0zKwTWAkf6ISqrkCcisq/162NHjt27oys3Xdq1i8fmzs2UOXNgzZpo1TvttFjW5ZJLoEOHfL8LkWRK1BIqZlZAdMn2Bv4LWAJsdvc9qUtWA11Tt7sCqwBSAXAL0aX7ca7qKyJS33XsCNddd+DHevSA4VV+vbjDBx/Ezh0TJ8K4cfD978MJJ0QrX//+0eLXvz9066YZvSL1Qc5CnrvvBYaaWRvgKaDfF31OMxsHjAPo0aPHF306EZEGyyy2XRsyBH75y1iseeJEeOONGOu3cWPm2rZtYfRoOOMMOP30GNcnInVPzifbu/tmM3sVGAW0MbPCVGteN6AsdVkZ0B1YnequPYKYgLH/c90L3AvRXZuL+ouIJJ0ZDB0aJW3DBvjwQ5g/H959F155JcIfxF68o0fHVm3pFr+uXdXaJ5JvuZp4cSSwOxXwmgEvAbcDY4Enqky8mOPuvzOz8cDgKhMvLnX3yw/1GhqTJyKSO+6wcCG8/HKUN96ATZsyj6cXbi4ujq7ho4+O0rMnDByoGb0iiZl4YWZDgAlAAdAImOjut5hZCbGESjtgNvANd/80teTKw8AwoBy40t2XHuo1FPJERPLHPSZzzJ+fKQsWwIoVsXDzzp2Zazt1ikkdl10GX/mKFnCWhikxIS8XFPJEROqmdABcuTJ27Jg8GZ59Ntbza98eLroIhg2L8HfUUVE6dYrWQHX5SlIp5GVBIU9EpP7YsSMWcJ40CaZMgYqKz17ToUOM80uXQYOiq7dVq9zXV6SmKeRlQSFPRKR+qqyM2btr18K6dXFcuzZa/T74INbx2749c32PHhH2Bg6MSR59+8ZEj86doXHj/L0PkWwkap08ERGRA2nUKHbZOPLIaLHbX2UlLF8egW/evEyZNg0+/XTfazt1isA3eDDcdBMcc0xO3oJInaSWPBERqZf27IGlS2HJEigrg9Wr41hWFrN9d+6MBZ1/9jPt3CF1j1ryREREDqKwMFrqDtRat24d/Pzn8LvfwYQJ0ap3/fWxV69IQ6GWPBERSaz58+EnP4nJHR06xO4cnTplZvJ27RrLuPTtq5m8kltqyRMREfkC+vePJVtefRUefBDWrInxfdOnxy4e6XaOo4+Gs8+Gc86J3TtatIgZwDt2xKSPTz6JCR8tW+bz3YhkRy15IiLSIO3dG4s1v/wyvPBCbNW2bVu06B3oV6NZhMbjj48yfHh0Fbdpo1ZAyZ6WUMmCQp6IiHwRu3bB22/Da69FyGvRApo3j2PjxrBoEbz3XpT16zM/16RJZhHnzp2hXz847TQ46aT4WZEDUcjLgkKeiIjkgnvM5J05M7p+16yJdf3WrIlSWhozf4uKYMSI6P4dMwZOPDHOiYBCXlYU8kREpC7Ytg3eeivGAb76aoTBykpo3RrOPBPOPTfG/3XunO+aSj5p4oWIiEg907IlnHVWFIAtWyLsPfdclEmT4ny/fjG7t+p+ve3aRSDcvTtTCgvhvPOgT5/8vSepn9SSJyIikiPuMGdOhL13381s47ZmTSzefCijRsHYsXD55dC2bW7qK7VH3bVZUMgTEZH6yh0qKqC8PFruiooyZfNmePTRWNT5ww9joscFF8QM35KSTGnTJt/vQrKhkJcFhTwREUkyd3j/fXjoIfjzn6P1r6q2bWOs37e/HZM9GjXKTz2lehTysqCQJyIiDcmWLbBsWezfu3Rp7O7x1FOwaVMs7nz11VF69sxzReWAFPKyoJAnIiIN3c6d8PTT8MADscizOwwZEgs3f+lLUYYMgWbN4vrKytjNY8eOzLqAkhsKeVlQyBMREclYuRIefhjeeANmzYKPP47zBQVwxBER7KpO9igsjHF+p50W5cQTFfpqk0JeFhTyREREDswdVq2KsDdrVkzmSLfcNW8eLXsffRRLvbz3Xmz51rhxtPwNHgyDBsHAgXHs2DHf7yYZFPKyoJAnIiLyxVVUwJtvRuB2MqDIAAAL10lEQVSbPh3mzYtZv2mtWkUorDoDuGVLOPVUuPDC2M5NO3t8PoW8LCjkiYiI1Dz3WMtv3rwoS5fGPr+7d2eOGzZEt/CuXbGUyznnxALORx0VS76kS9OmMSmkSZN8v6v8044XIiIikldmsQVb585w+ukHv66iIiZ7TJkCzz4ba/sdSFERDB0a4/9GjIjSu7da/2qDWvJERESkRu3dC3PnxjIvn34aZdeumOwxd27s9jFzZgRDiCB51FHQrVts9datW4wFPOusaPlLIrXkiYiISL1TUADHHnvoayorobQ0JnosXQqrV0dZtAimTYOtW+O6/v0j7J19duY5KyujuEdAbNYsJpA0bRr3JaglT0REROoUd1iwAF54Icprr0VrYHU0awbt28OXvwxjxsTuH8XFtVvfw6GJF1lQyBMREUmmHTvg9dejxa9Royhmcay6oHP6WFYWs4PXro2fLy6O2b8lJZnu4PSxdev8vCd114qIiEiD17x5dNdmwz22eps2DaZOjckg69d/9rr27aFXr5j8kS6jRsWxvlNLnoiIiDQIO3fGos9lZZkxgEuXwuLFUVaujJbBO+6AG2+s3bqoJU9ERESkhjRtGl22JSUHfnzXLli+PH9duDVNIU9ERESE2MrtmGPyXYua0yjfFRARERGRmqeQJyIiIpJACnkiIiIiCaSQJyIiIpJACnkiIiIiCaSQJyIiIpJAOQl5ZtbdzF41sw/NbJ6ZXZ86387MXjazRalj29R5M7PfmtliM5tjZsflop4iIiIiSZGrlrw9wA/dfQAwEhhvZgOAnwJT3b0PMDV1H+AcoE+qjAPuzlE9RURERBIhJyHP3de4+/up2xXAfKArcBEwIXXZBODi1O2LgIc8TAfamFnnXNRVREREJAlyPibPzHoCw4AZQCd3X5N6aC3QKXW7K7Cqyo+tTp0TERERkWrIacgzs5bAE8AP3H1r1cfc3QHP8vnGmdlMM5u5YcOGGqypiIiISP2Ws5BnZkVEwHvE3Z9MnV6X7oZNHdenzpcB3av8eLfUuX24+73uPtzdhx955JG1V3kRERGReiZXs2sNuB+Y7+6/rvLQZGBs6vZY4Jkq57+VmmU7EthSpVtXRERERD5HYY5e5yTgm8AHZva/qXM3AbcBE83sWmAFcHnqseeAc4HFwA7gmhzVU0RERCQRchLy3P1NwA7y8JgDXO/A+FqtlIiIiEiCaccLERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJIIU8ERERkQRSyBMRERFJoJyEPDP7o5mtN7O5Vc61M7OXzWxR6tg2dd7M7LdmttjM5pjZcbmoo4iIiEiS5Kol70Hg7P3O/RSY6u59gKmp+wDnAH1SZRxwd47qKCIiIpIYOQl57v46UL7f6YuACanbE4CLq5x/yMN0oI2Zdc5FPUVERESSIp9j8jq5+5rU7bVAp9TtrsCqKtetTp0TERERkWoqzHcFANzdzcyz/TkzG0d06QJsM7PSmq3ZZ3QAPq7l15DDo8+mbtLnUnfps6mb9LnUXTX92Rxdg891QPkMeevMrLO7r0l1x65PnS8Dule5rlvq3Ge4+73AvbVbzQwzm+nuw3P1elJ9+mzqJn0udZc+m7pJn0vdVR8/m3x2104GxqZujwWeqXL+W6lZtiOBLVW6dUVERESkGnLSkmdmjwKnAh3MbDVwM3AbMNHMrgVWAJenLn8OOBdYDOwArslFHUVERESSJCchz92vOshDYw5wrQPja7dGhy1nXcOSNX02dZM+l7pLn03dpM+l7qp3n41FphIRERGRJNG2ZiIiIiIJpJBXTWZ2tpmVprZb++nn/4TUBjPrbmavmtmHZjbPzK5PnT/gNnmSe2ZWYGazzewvqfvFZjYj9d153Mwa57uODY2ZtTGzSWa2wMzmm9kofWfqBjP7h9S/ZXPN7FEza6rvTH4kcQtWhbxqMLMC4L+ILdcGAFeZ2YD81qrB2gP80N0HACOB8anP4mDb5EnuXQ/Mr3L/duA37t4b2ARcm5daNWz/Abzg7v2AY4nPR9+ZPDOzrsDfA8PdfRBQAFyJvjP58iAJ24JVIa96RgCL3X2pu+8CHiO2X5Mcc/c17v5+6nYF8cuqKwffJk9yyMy6AecB96XuGzAamJS6RJ9NjpnZEcApwP0A7r7L3Tej70xdUQg0M7NCoDmwBn1n8iKJW7Aq5FWPtlqrg8ysJzAMmMHBt8mT3LoT+DFQmbrfHtjs7ntS9/Xdyb1iYAPwQKob/T4za4G+M3nn7mXAvwEriXC3BZiFvjN1Sb3eglUhT+olM2sJPAH8wN23Vn0stQyPpo3nmJmdD6x391n5rovsoxA4Drjb3YcB29mva1bfmfxIje+6iAjiXYAWfLa7UOqI+vg9UcirnmpvtSa1z8yKiID3iLs/mTq9Lt1Uvt82eZI7JwEXmtlyYkjDaGIsWJtUVxTou5MPq4HV7j4jdX8SEfr0ncm/04Fl7r7B3XcDTxLfI31n6o6DfU/qRS5QyKue94A+qRlPjYmBsZPzXKcGKTXG635gvrv/uspDB9smT3LE3f+fu3dz957Ed2Sau38deBW4LHWZPpscc/e1wCoz65s6NQb4EH1n6oKVwEgza576ty392eg7U3fU6y1YtRhyNZnZucR4owLgj+7+z3muUoNkZicDbwAfkBn3dRMxLm8i0IPUNnnuvv8AWskRMzsV+JG7n29mJUTLXjtgNvANd/80n/VraMxsKDEZpjGwlNgushH6zuSdmf0CuIJYOWA2cB0xtkvfmRyrugUrsI7YgvVpDvA9SYXyu4ju9R3ANe4+Mx/1PhSFPBEREZEEUnetiIiISAIp5ImIiIgkkEKeiIiISAIp5ImIiIgkkEKeiIiISAIp5ImIfAFm5mbWO9/1EBHZn0KeiCSKmS03s0/MbFuVcle+6yUikmuFn3+JiEi9c4G7v5LvSoiI5JNa8kSkQTCzq83sLTO7y8y2mNkCMxtT5fEuZjbZzMrNbLGZfafKYwVmdpOZLTGzCjObZWZV96083cwWmdlmM/uv1Gr4mFlvM3st9Xofm9njOXzLItLAqSVPRBqSE4BJxLZFlwJPmllxajuvx4C5QBegH/CymS1x92nADcBVwLnAQmAIsZVR2vnA8UBrYBYwBXgB+CXwEnAasaXY8Np+gyIiadrWTEQSxcyWEyFuT5XTNwK7gVuBrp76h8/M3gX+E/gfYDnQxt0rUo/9C9DZ3a82s1Lgx+7+mY3izcyBL7v7m6n7E4H33f02M3sI2Anc4u6ra+HtiogclLprRSSJLnb3NlXKH1Lny3zfv2xXEC13XYDydMCr8ljX1O3uwJJDvN7aKrd3AC1Tt38MGPCumc0zs28f5vsREcmaQp6INCRd0+PlUnoAH6VKOzNrtd9jZanbq4Be2b6Yu6919++4exfgu8DvtNyKiOSKQp6INCQdgb83syIz+yrQH3jO3VcBbwP/YmZNzWwIcC3wp9TP3Qf80sz6WBhiZu0/78XM7Ktm1i11dxPgQGVNvykRkQPRxAsRSaIpZra3yv2XgWeAGUAf4GNgHXCZu29MXXMVcA/RqrcJuLnKMiy/BpoQkyg6AAuAS6pRj+OBO83siNTrXe/uS7/IGxMRqS5NvBCRBsHMrgauc/eT810XEZFcUHetiIiISAIp5ImIiIgkkLprRURERBJILXkiIiIiCaSQJyIiIpJACnkiIiIiCaSQJyIiIpJACnkiIiIiCaSQJyIiIpJA/x/6+FW4omK7CwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix572WnaF2Gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f79022-a169-4375-e76b-83b482edc504"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/tut3-model-1c.pt'))\n",
        "\n",
        "test_loss_1b = evaluate(model, test_loader , criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss_1b:.3f} | Test PPL: {math.exp(test_loss_1b):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 6.108 | Test PPL: 449.457 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6WzonRrGJYo"
      },
      "source": [
        "#functions for human evaluation on test set\n",
        "\n",
        "def int_2_words( i_text , vocab ) :\n",
        "\n",
        "  text_list = []\n",
        "  for i_idx in i_text :\n",
        "    i_word =  vocab[i_idx] \n",
        "\n",
        "    if(i_word=='<pad>'):\n",
        "      continue\n",
        "\n",
        "    text_list.append( i_word )\n",
        "\n",
        "    if(i_word==\"END\"):\n",
        "      break\n",
        "\n",
        "  text=\" \".join(text_list)\n",
        "\n",
        "  return text\n",
        "\n",
        "def generate_sentence(model, iterator):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    pred_trg_pairs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src , trg) in enumerate(iterator):\n",
        "\n",
        "            #move to device\n",
        "            src , trg = src.to(device) , trg.to(device)\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[:,1:]\n",
        "            trg = trg[:,1:]\n",
        "\n",
        "            output = output.detach().cpu().numpy()\n",
        "            trg = trg.detach().cpu().numpy()\n",
        "            pred= np.argmax( output , axis = -1 )\n",
        "\n",
        "            for i_pred , i_trg in zip( pred , trg  ):\n",
        "\n",
        "              trg_sen = int_2_words( i_trg , int_to_vocab_trg  )\n",
        "              pred_sen = int_2_words( i_pred , int_to_vocab_trg )\n",
        "\n",
        "              print( \"Target Sentence : {} \\n\".format( trg_sen )  )\n",
        "              print( \"Predicted Sentence : {} \\n\".format( pred_sen )  )\n",
        "\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKL39ULGGOXF"
      },
      "source": [
        "#evaluate on test set\n",
        "generate_sentence(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}